1
00:00:00,800 --> 00:00:06,720
Welcome back. So now that you already know what kind of fundamental data structures you have which

2
00:00:06,720 --> 00:00:11,920
are native to python basically data structures that you can directly take for granted in python

3
00:00:11,920 --> 00:00:17,840
we are in this video going to introduce a library called pandas. Pandas stands for panel data

4
00:00:17,840 --> 00:00:25,760
analysis which basically uses lists dictionaries and arrays from python or numpy to create something

5
00:00:25,840 --> 00:00:34,400
called a data frame. A data frame is a supercharged data storage structure which is available through

6
00:00:34,400 --> 00:00:41,040
pandas and you can think of it as excel for python so if you know how to use pandas very well you

7
00:00:41,040 --> 00:00:47,040
should be able to do pretty much everything that excel can do and more in a programmatic pythonic

8
00:00:47,040 --> 00:00:54,320
way. So let's get started first of all in order to run the notebook we'll have to import a bunch of

9
00:00:54,320 --> 00:00:58,480
modules so we are basically going to import pandas and we're also going to do some

10
00:01:00,160 --> 00:01:03,840
plotting so we are going to import the plotting library which is called matplotlib

11
00:01:04,960 --> 00:01:11,920
let's run that so essentially pandas lets us read process and analyze any kind of tabular data

12
00:01:11,920 --> 00:01:18,400
so when we say read what we mean is that pandas has parsers for multiple different file formats

13
00:01:18,400 --> 00:01:24,080
you can read from csv files you can read from excel sheets you can read from databases

14
00:01:24,160 --> 00:01:30,000
and so many other sources you can process that data once you have taken care of the file io not

15
00:01:30,000 --> 00:01:33,280
only can it read from all of these different formats it can actually also write back to all

16
00:01:33,280 --> 00:01:38,880
of these different formats so it takes care of files file io as well as it can do the processing

17
00:01:38,880 --> 00:01:44,720
of the data which is basically most of the bulk of pandas functionality essentially processing

18
00:01:44,720 --> 00:01:50,400
cleaning manipulation and all of that and finally it also has a few things which we can use for

19
00:01:50,400 --> 00:01:56,560
analysis this is where things get into the mathematics and statistics part so the univariate

20
00:01:56,560 --> 00:02:02,400
bivariate multivariate analysis techniques that we have briefly covered in an earlier video you'll

21
00:02:02,400 --> 00:02:09,360
see that most of them are doable right within pandas so the primary entry point into pandas is

22
00:02:09,360 --> 00:02:14,480
like i said data frame so let's take a look in a little more detail on what a data frame is

23
00:02:15,040 --> 00:02:19,040
in general you can just consider a data frame as the python equivalent of

24
00:02:19,680 --> 00:02:25,280
excel sheet so these are basically labeled two-dimensional data structures what do we

25
00:02:25,280 --> 00:02:31,040
mean by two-dimensional data structures is that we saw that we had lists which were simply indexable

26
00:02:31,040 --> 00:02:36,240
by a certain position that is if you had to access a particular value in a list you only had to know

27
00:02:36,240 --> 00:02:41,600
its position whereas most of our data as you know is going to be tabular so you can access a particular

28
00:02:41,600 --> 00:02:46,640
cell value by its row position as well as its column position and these don't even have to be

29
00:02:46,640 --> 00:02:53,520
positions we have to have them as you know we can have them as names not necessarily integers

30
00:02:53,520 --> 00:03:00,160
or numbers so that's what we mean by saying that it's a labeled two-dimensional data structure what

31
00:03:00,160 --> 00:03:05,120
it means is that the rows have their own labels columns have their own labels just like a dictionary

32
00:03:05,120 --> 00:03:10,800
has labels and values against them or keys and values against them pandas rows are also like

33
00:03:10,800 --> 00:03:14,480
dictionaries and columns are also like dictionaries you're going to see that in a little more detail

34
00:03:14,480 --> 00:03:20,640
in just a little bit and it's basically in spirit very similar to an excel worksheet or a relational

35
00:03:20,640 --> 00:03:26,320
database table so essentially pretty much whatever you can do with rows and columns is what you can

36
00:03:26,320 --> 00:03:34,720
do with pandas and by now we have also spoken in quite some amount of detail about the details of

37
00:03:34,800 --> 00:03:41,440
tabular data but let's also revise it just for the sake of completeness so we know that

38
00:03:41,440 --> 00:03:46,000
each row represents one sample out of all the data points that you've collected each column

39
00:03:46,000 --> 00:03:50,880
contains a different variable that describes the sample or which is rows essentially each column is

40
00:03:50,880 --> 00:03:55,120
an attribute of the thing that you're measuring and the data in every column is usually the same

41
00:03:55,120 --> 00:03:59,200
data type that is if you're measuring somebody's height then the whole column is more likely to

42
00:03:59,200 --> 00:04:05,600
contain numbers rather than strings and so on so that's basically what creates a data frame it's

43
00:04:05,600 --> 00:04:11,120
essentially just a table and can be created in many ways such as using another data frame or you

44
00:04:11,120 --> 00:04:16,160
know using a numpy array or a composite of arrays that has two dimensional shapes so as we've seen

45
00:04:16,160 --> 00:04:21,440
in the previous video we have a bunch of two dimensional shapes for example the nested list

46
00:04:21,440 --> 00:04:27,120
example if you remember which was a list of lists and each internal list had the same amount of

47
00:04:27,120 --> 00:04:32,800
elements in it so that represents a table at the same time a dictionary which has keys as the column

48
00:04:32,800 --> 00:04:38,400
headers and the values as lists which contain the column values that also represents essentially a

49
00:04:38,400 --> 00:04:44,080
two-dimensional data set which can be used to create a data frame you can use a panda series

50
00:04:44,080 --> 00:04:50,320
also now a series is essentially just one column or one row of a data frame so if a data frame is

51
00:04:50,320 --> 00:04:55,600
a two-dimensional structure a series is essentially it's one-dimensional equivalent within the pandas

52
00:04:55,600 --> 00:05:01,680
ecosystem it can be produced from a file such as a csv file or an excel file it can be also

53
00:05:01,680 --> 00:05:08,800
created from a table and we have already covered how it can be created from a dictionary so these

54
00:05:08,800 --> 00:05:15,200
are some common operations with pandas let's say that this is one particular data set that we have

55
00:05:15,200 --> 00:05:20,080
which contains again it's actually a dictionary so you can tell that from the braces we haven't

56
00:05:20,080 --> 00:05:25,040
closed a bunch of things within braces which means that it's a dictionary we have keys colon

57
00:05:25,040 --> 00:05:30,640
values so essentially this is a dictionary which has three key value pairs the first is name the

58
00:05:30,640 --> 00:05:35,120
second is score and the third is attempt and if you look at the values of each of these keys they

59
00:05:35,120 --> 00:05:41,120
are essentially lists of the same length whatever is the length of this particular well sequence of

60
00:05:41,120 --> 00:05:46,800
names that is the same as the length of the sequence of scores and of the attempts and let's

61
00:05:46,800 --> 00:05:52,000
say that we have another list which is just a list of letters from a to j so that's 10 and all

62
00:05:52,000 --> 00:05:59,360
of these are also 10 so essentially these are interpreted by pandas as column headers and the

63
00:05:59,360 --> 00:06:05,360
values of a given column we can just put the dictionary within the pd.dataFrame caller over

64
00:06:05,360 --> 00:06:12,080
here and we can say that we want the index to be these labels so let's just run that and we see

65
00:06:12,080 --> 00:06:17,600
that we get a data frame this is essentially our data which is tabular and we saw that name

66
00:06:17,600 --> 00:06:22,320
which was a key became a column header score which was another key became the next and attempts became

67
00:06:22,320 --> 00:06:29,360
the last and the values inside each of these values they became the contents of the column itself

68
00:06:30,400 --> 00:06:35,040
now here's how you do some of the common operations within a data set this is the whole

69
00:06:35,040 --> 00:06:40,000
data set containing 10 rows if we just want to see the top few rows we can just do a df.head

70
00:06:40,000 --> 00:06:45,520
now df.head by default when you call it it's going to show us the top five rows but we can

71
00:06:45,520 --> 00:06:50,800
give it any number over here i can say just show me the top two rows or i can say show me all 10

72
00:06:50,800 --> 00:06:56,480
rows or i can just say show me eight rows so i can just pass it an integer and it's going to show me

73
00:06:56,480 --> 00:07:04,720
the top k columns or the top k rows of a particular data frame the same thing i can also do using an

74
00:07:04,720 --> 00:07:11,040
iloc function iloc is essentially integer location so essentially i'm saying that give me the first

75
00:07:11,040 --> 00:07:17,280
three rows and that's it so that basically gives me the first three rows this is how you can slice

76
00:07:17,280 --> 00:07:22,960
out different rows of a data frame in order to get a column i can just remember the data frames

77
00:07:22,960 --> 00:07:27,280
also act like dictionaries so essentially just like you get a key value pair from a dictionary

78
00:07:27,280 --> 00:07:31,840
this is how you get a column from a data frame this is going to give me the score column out of

79
00:07:31,840 --> 00:07:38,080
the data set which as you see is 12.5 9.0 16.5 all the way up to 7.5 so let's try that

80
00:07:39,040 --> 00:07:44,400
and there it is you see in both those cases whenever we try to get a row the index which

81
00:07:44,400 --> 00:07:49,760
is abcdef all the way up to j remains the same what that means is that each location

82
00:07:49,760 --> 00:07:56,720
instead of being indexed by a number is being indexed by a letter right so that simply is

83
00:07:56,720 --> 00:08:02,720
useful because sometimes strings or names are easier to remember than numbers so just like in

84
00:08:02,720 --> 00:08:08,000
a list you have positioning by numbers here you can have positioning by arbitrary strings so those

85
00:08:08,000 --> 00:08:13,440
are called the index of a data frame you can also filter a column which says that you know let's say

86
00:08:14,400 --> 00:08:20,000
i want somebody whose name is emily and i want to take that person's score so we know that there

87
00:08:20,000 --> 00:08:25,760
is exactly one row which has name as emily and i want to get this particular value operator so

88
00:08:25,760 --> 00:08:31,680
i can use that as a filter i can say that filter the data frame df such that the name column equals

89
00:08:31,680 --> 00:08:38,160
emily and from that get the score out so there it is that basically gives me the score of emily

90
00:08:39,040 --> 00:08:44,240
easier way to break that down would be something like this let's say that i simply do df.name

91
00:08:44,240 --> 00:08:50,720
equal to equal to what this is going to return is a set of boolean values which is whether the name

92
00:08:50,720 --> 00:08:56,000
is emily whether the name is emily for this particular row and for that row or not and

93
00:08:56,000 --> 00:09:01,920
essentially there is only one row where it is true and when i use this to slice my data frame

94
00:09:01,920 --> 00:09:08,160
in general it's basically going to give me all those rows where the name is emily in this case

95
00:09:08,160 --> 00:09:13,920
it's just one row so that gives me the first row the only row which contains the name as emily

96
00:09:13,920 --> 00:09:19,680
and out of this like we know this is how we get a score or we get a particular column out

97
00:09:20,400 --> 00:09:28,160
we get the column so there it is it's basically 9.0 and here's how we edit a cell when you want

98
00:09:28,160 --> 00:09:34,320
to change the particular value in a cell we just do an loc operation on it which stands for location

99
00:09:34,320 --> 00:09:38,960
so what this instruction says is that find the location d and then find the column score and

100
00:09:38,960 --> 00:09:44,560
set its value to something so what this means is that in my original data frame the d column is

101
00:09:44,560 --> 00:09:51,760
james and the score is 6.0 what i'm going to do is score set the score to 11.5 so when we do this

102
00:09:51,760 --> 00:09:57,440
operation sorry when we do this operation and look at the data frame again we'll see that the score

103
00:09:57,440 --> 00:10:04,960
of james is going to be 11.5 so let's try that there it is so we see that the score of james

104
00:10:04,960 --> 00:10:10,560
has become 11.5 in order to do that remember what we had to do was find the index value which was d

105
00:10:10,560 --> 00:10:17,360
and find the score or the column that we wanted to change and we just assign the desired value

106
00:10:17,360 --> 00:10:23,200
to this output we have already seen some examples of logical filtering but let's take a look at some

107
00:10:23,200 --> 00:10:30,160
of them again let's say that we want to take all the elements or all the rows of the data frame

108
00:10:30,160 --> 00:10:36,080
where the score is greater than 15 or less than or equal to 20 in the sense that we want to find

109
00:10:36,080 --> 00:10:41,440
all the scores all the people who scored between 15 and 20 marks both values included so what we

110
00:10:41,440 --> 00:10:47,760
are saying is that create a boolean value just like we had a boolean value over here

111
00:10:49,280 --> 00:10:54,000
which contains true for all the scores which are greater than or equal to 15 and do do an

112
00:10:54,000 --> 00:10:59,760
logical and with another set of boolean values which have a score of less than or equal to 20

113
00:10:59,760 --> 00:11:06,240
so when we run this we see that katherine michael and jones are the only three people who got scores

114
00:11:06,240 --> 00:11:11,440
between 15 and 20 and if you verify this from the data you will see that it is actually correct

115
00:11:12,320 --> 00:11:16,800
the next step would be to do some sort of arithmetic on the column this is useful when

116
00:11:16,800 --> 00:11:22,320
we are trying to calculate our univariate statistics or aggregations and grouping which

117
00:11:22,320 --> 00:11:28,720
we will consider later in this particular video you can just take the column remember that this

118
00:11:28,720 --> 00:11:33,920
is how you access a column and call a sum or a mean on it so let's say the total number of attempts

119
00:11:33,920 --> 00:11:39,840
is basically 90 so this is how you can do it and if you try to take the average score of the entire

120
00:11:39,840 --> 00:11:46,880
batch that's 12.75 now in order to motivate some of these examples further what we are going to do

121
00:11:46,880 --> 00:11:55,440
is look at a case study so kaggle is an online platform which runs competitions which runs

122
00:11:55,440 --> 00:12:01,120
basically data science competitions if you go to this compete tab over here you will see that there

123
00:12:01,120 --> 00:12:07,200
are a bunch of competitions that kaggle hosts essentially what they do is that if a company

124
00:12:07,200 --> 00:12:11,360
has a lot of data but they don't know what to do with that data and they want that data analyzed

125
00:12:11,360 --> 00:12:16,800
or they want some sort of predictive models built with that data they will host a competition for

126
00:12:16,800 --> 00:12:23,520
that in exchange for a fee from the company and what what they do then is that they invite

127
00:12:23,520 --> 00:12:29,440
practitioners such as yourself to compete on these data sets to come up with a best possible model

128
00:12:29,440 --> 00:12:35,040
and whoever is the winner gets a little prize from the organizers and from kaggle and the

129
00:12:35,040 --> 00:12:40,560
organizers get the algorithm and the winner gets the money so that's how basically this works it's

130
00:12:40,560 --> 00:12:47,200
also you know a way of crowd sourcing solutions and what they do is that this is by the way a

131
00:12:47,200 --> 00:12:52,960
very well known data science platform a lot of data sets are available here a lot of competitions

132
00:12:52,960 --> 00:12:58,880
are available here it's also you know quite addictive in a way because people can waste a

133
00:12:58,880 --> 00:13:08,240
lot of time on this so what they do is that they conduct a survey every year where they talk to

134
00:13:08,240 --> 00:13:14,160
all the people who are enrolled on the kaggle platform and they talk to those people about their

135
00:13:14,160 --> 00:13:21,680
preferences and what sort of working environment they have what sort of jobs they do do they use

136
00:13:21,680 --> 00:13:25,280
machine learning or not if they use machine learning audit analysis what algorithms they

137
00:13:25,280 --> 00:13:28,960
use what software they use a bunch of different things so essentially what we're going to do in

138
00:13:28,960 --> 00:13:35,200
this particular case study is take the data from this survey and try to see what sort of analysis

139
00:13:35,200 --> 00:13:40,800
we can do on that so essentially this is the file which contains the survey results let's read that

140
00:13:40,800 --> 00:13:45,600
file it's a csv file which means it's a comma separated value file we are going to use the

141
00:13:45,600 --> 00:13:51,360
read csv function again included with pandas to read that data into a data frame and let's

142
00:13:51,680 --> 00:13:55,920
as we know the head function is going to give us the top five rows let's take a look at that

143
00:13:57,600 --> 00:14:02,720
so there it is so this basically is our header it says that i have i'm looking at five rows because

144
00:14:02,720 --> 00:14:07,680
i called df.head without any function and it gives me five rows by default i can change that

145
00:14:07,680 --> 00:14:14,000
whenever i want and it has 246 columns why it has 246 columns is because the survey contains

146
00:14:14,000 --> 00:14:20,000
a lot of questions it's a fairly involved and a fairly detailed survey it'll take about a good

147
00:14:20,960 --> 00:14:26,720
half an hour to 45 minutes for somebody to sit down and finish the survey but at the same time

148
00:14:26,720 --> 00:14:33,680
it's very valuable because this is a good way for the data science and the data analytics community

149
00:14:33,680 --> 00:14:41,200
to get more information out there about themselves and about the different opportunities and what

150
00:14:41,200 --> 00:14:47,120
sort of work people do and so on so as we see the very first column essentially is time from start

151
00:14:47,120 --> 00:14:53,600
to finish in seconds and every row is basically one respondent of the survey so basically whoever

152
00:14:53,600 --> 00:14:59,840
this person was that person took 510 seconds to fill the survey and their age was between 20 and

153
00:14:59,840 --> 00:15:07,520
24 years of 22 and 24 years that person was a male and this is one particular question that

154
00:15:07,520 --> 00:15:12,400
they did not choose to answer because this was basically what is your gender and you know the

155
00:15:12,480 --> 00:15:16,080
option here is prefer to self-describe since they've already sent their mail

156
00:15:16,080 --> 00:15:20,960
they wouldn't want to take this particular option they live in france they have a master's degree

157
00:15:22,160 --> 00:15:28,400
they the job title is software engineer they work in a company where there are about thousand to

158
00:15:28,400 --> 00:15:33,920
ten thousand employees and so on so that's basically the data set that we have so let's take a you know

159
00:15:35,600 --> 00:15:39,600
a little bit of time in trying to understand the data you already know the shape of the data there

160
00:15:39,600 --> 00:15:44,800
are nineteen thousand seven hundred and eighteen surveys by the way when you call df dot shape on

161
00:15:44,800 --> 00:15:49,040
a particular data frame it's going to give you two numbers the first will always be the number of

162
00:15:49,040 --> 00:15:54,080
rows and the second will always be the number of columns here also we can see that we have five

163
00:15:54,080 --> 00:16:00,320
rows and 246 columns but that's because we only did df dot head and that only lets us see five

164
00:16:01,040 --> 00:16:05,920
rows at a time so the overall number of rows is nineteen thousand seven hundred and eighteen which

165
00:16:05,920 --> 00:16:11,840
means that in this survey nineteen thousand seven hundred and eighteen data analysts from all over

166
00:16:11,840 --> 00:16:17,680
the world participated now they will not necessarily identify data as data analysts but nineteen

167
00:16:17,680 --> 00:16:24,320
thousand seven hundred and eighteen kaggle users participated in the survey so and we have 246

168
00:16:24,320 --> 00:16:29,040
columns which are basically a combination of multiple questions that were asked in the survey

169
00:16:29,040 --> 00:16:34,640
and all the possible answers that you could have given to that survey so let's take a look at this

170
00:16:35,280 --> 00:16:39,520
how do we interpret the dimensions of this data well it's obvious there are nineteen hundred seven

171
00:16:39,520 --> 00:16:45,040
nineteen thousand seven hundred and eighteen participants and there are 246 response columns

172
00:16:45,920 --> 00:16:51,440
each row is a question in the survey this option is clearly not correct it is a participant's

173
00:16:51,440 --> 00:16:56,320
response to the survey essentially and each column is essentially a question in the survey

174
00:16:56,320 --> 00:17:02,640
and also you know the set of all answers given to a given to a given question so basically this

175
00:17:02,640 --> 00:17:10,880
column the third column is essentially the genders of all the people who have participated in this

176
00:17:10,880 --> 00:17:15,600
particular survey right so how do we get the rows let's say that i want to just get the first row

177
00:17:15,600 --> 00:17:22,160
so i set i equal to zero and i do df dot log of i so that's my first row now you see there is

178
00:17:22,160 --> 00:17:26,240
something really funny with this data set the headers are there and then in the first line

179
00:17:26,240 --> 00:17:32,640
we have the header again essentially you see you have things like q1 q2 q2 other text q3 q4

180
00:17:32,640 --> 00:17:38,400
so essentially the header is basically a bunch of codes that are given to the question whereas

181
00:17:38,400 --> 00:17:44,000
in the first row you see that that is the verbatim verbose version of the whole question this is the

182
00:17:44,000 --> 00:17:49,840
question the text of the question as it appeared in the survey itself why they have given codes

183
00:17:49,840 --> 00:17:54,080
is that because you know the questions might become too long and therefore it might just be

184
00:17:54,080 --> 00:18:00,480
easy to deal to refer to them with codes right so there is our first column it's basically the

185
00:18:01,600 --> 00:18:07,840
code and the meaning of that question essentially the question code and the question itself that's

186
00:18:07,840 --> 00:18:12,880
the pairing we have here now how do we get a list of columns from a data frame we just do a df dot

187
00:18:12,880 --> 00:18:19,520
columns so the columns attribute of the data frame variable which is df is just going to give me all

188
00:18:19,520 --> 00:18:24,880
the columns that we have and these are all the columns we have we have see time taken from start

189
00:18:24,880 --> 00:18:31,360
to finish q1 q2 q2 other text q3 q4 q5 q5 had other text because you know sometimes you have

190
00:18:31,360 --> 00:18:35,520
questions and surveys where it's an mcq kind of a question but at the same time there'll be a field

191
00:18:35,520 --> 00:18:40,880
called other or something like that where you can add in free text instead of choosing one of the

192
00:18:40,880 --> 00:18:47,840
predefined choices that are given to you so that's basically captured under the other text column

193
00:18:47,840 --> 00:18:51,920
for that particular question and then there are multiple questions which are you know in multiple

194
00:18:51,920 --> 00:18:57,280
parts so for example we can see here that question 34 had 12 parts so we have we can see that there

195
00:18:57,280 --> 00:19:04,560
was part four five six some of them are not shown here because this is a list which has 246 elements

196
00:19:04,560 --> 00:19:08,720
in it and python chooses not to show it we can actually of course convert it to a list

197
00:19:09,600 --> 00:19:14,960
and actually see the whole list but that would be too long to pass and that's why it's more or less

198
00:19:14,960 --> 00:19:20,640
meaningless you can just look at the columns to get a summary idea of whatever number of columns

199
00:19:20,640 --> 00:19:26,480
there are so we have we the point is that there are questions and some of them have multiple

200
00:19:26,480 --> 00:19:32,720
choice answers and some of them also have text free text answers essentially so we know how to

201
00:19:32,720 --> 00:19:38,240
select a particular column we just say that my column name is time from start to finish in seconds

202
00:19:38,320 --> 00:19:46,560
and i just do i just get that column out by putting a bunch of putting a pair of square brackets

203
00:19:46,560 --> 00:19:51,040
around it and calling it on the dictionary just like you get a key value pair out of a dictionary

204
00:19:51,040 --> 00:19:57,680
similarly you get a column out of a data frame so basically i know that my column is time from start

205
00:19:57,680 --> 00:20:03,520
to finish in seconds i put that string in a variable called call name and i just access the

206
00:20:03,520 --> 00:20:08,240
call name from the data frame and that's it so i see that the zero value is duration and seconds

207
00:20:08,240 --> 00:20:13,680
the first value is 510 and so on blah blah blah right so basically that's the

208
00:20:17,840 --> 00:20:21,520
time that every participant took to finish the survey now here's a question

209
00:20:22,720 --> 00:20:28,000
how is this column useful to somebody who is analyzing this data we want to analyze this data

210
00:20:28,640 --> 00:20:34,560
we want to figure out what are the interesting patterns that are available within this data set

211
00:20:34,560 --> 00:20:39,600
which is coming from the responses of data science practitioners from all over the world

212
00:20:39,600 --> 00:20:42,560
so clearly there is a lot of inherent value on this we might want to see

213
00:20:43,200 --> 00:20:46,960
what are their preferences how much do they earn what are their qualifications and so on

214
00:20:46,960 --> 00:20:50,880
a bunch of those things are there so one of the columns we're considering first

215
00:20:50,880 --> 00:20:56,240
is the time column the duration think a bit about how this might be useful

216
00:20:58,000 --> 00:21:01,920
feel free to pause the video and when you have thought about it when you have say a couple of

217
00:21:01,920 --> 00:21:06,880
ideas about how a duration might be useful in analyzing this data resume the video

218
00:21:09,040 --> 00:21:15,280
well if you think about it these are survey results and unfortunately most of us do not

219
00:21:15,280 --> 00:21:21,760
take surveys very seriously we either just submit them if they are short enough and if

220
00:21:21,760 --> 00:21:26,320
it's convenient for us but otherwise we just skip past them and you know that evidence for that can

221
00:21:26,320 --> 00:21:32,480
be seen here you see that there are people who have finished the survey in 83 and 84 seconds

222
00:21:32,480 --> 00:21:38,480
which is just even less than two minutes it's barely one and a half minutes so for a survey

223
00:21:38,480 --> 00:21:45,840
that has about 246 data points to capture about you there is no way that you can finish that

224
00:21:45,840 --> 00:21:51,600
survey properly in 83 or 84 seconds some of the other values yeah they do look a little

225
00:21:52,240 --> 00:22:00,480
reasonable but at the same time we see that we have terribly large values as well 9,195 seconds

226
00:22:00,480 --> 00:22:05,680
so these are probably people who opened up the survey in a browser tab and then forgot about it

227
00:22:05,680 --> 00:22:11,760
and only later remembered it and then submitted the survey so you know what's happening here is

228
00:22:11,760 --> 00:22:17,600
that we have people who either just opened the survey and ignored it completely or people who

229
00:22:17,600 --> 00:22:22,320
opened it and forgot about it with of course a bunch of values in between so let's take a look

230
00:22:22,320 --> 00:22:30,000
at what are the minimum and maximum values in this particular column and that raises an error

231
00:22:30,000 --> 00:22:34,880
let's take a look at the error it basically says that less than or equal to is not supported

232
00:22:34,880 --> 00:22:39,600
between instance of string and it so it's essentially in order to calculate the minimum

233
00:22:39,600 --> 00:22:45,840
or the maximum somewhere internally it has to perform this less than or equal to operation

234
00:22:45,840 --> 00:22:50,080
and it says that it's not supported between instances of string and it's what it means

235
00:22:50,080 --> 00:22:57,440
is that it doesn't know how to compare strings and ends it you know if you give it a pair of

236
00:22:57,440 --> 00:23:02,320
two integers then it knows one of them is greater than or equal to the other or not

237
00:23:02,320 --> 00:23:08,000
but when some of the columns are strings that goes for a toss it doesn't know how to compare

238
00:23:08,000 --> 00:23:13,440
strings and integers and why is this happening well look at you know if we can think take a look

239
00:23:13,440 --> 00:23:18,480
at the first row you see that the first row is a string and everything else is a number so what

240
00:23:18,480 --> 00:23:23,840
we need to do in order to find the minimum and maximum is basically to omit this particular row

241
00:23:23,840 --> 00:23:30,240
and look only at the remainder of rows so let's try that so when we see df.head we see that

242
00:23:31,280 --> 00:23:38,800
entire first row for all the questions is basically just the question reworded in the sense that our

243
00:23:38,800 --> 00:23:44,640
original header is basically just text and the first row is just you know another piece

244
00:23:44,640 --> 00:23:48,960
of text which actually contains the whole questions and the real data of the survey

245
00:23:48,960 --> 00:23:52,640
starts from the first row onwards not from the zeroth row so what we need to do is omit this

246
00:23:52,640 --> 00:24:00,560
whole row and somehow keep a mapping to the headers themselves so in order to debug this error that

247
00:24:00,560 --> 00:24:05,760
we got say it says that it's a type error which means a type error usually occurs in pythons when

248
00:24:05,760 --> 00:24:12,160
you're trying when you're trying to do operations on things that don't have the correct type for it

249
00:24:12,160 --> 00:24:16,240
and that is actually true over here so when you do a less than or equal to it is only supported

250
00:24:16,240 --> 00:24:21,120
between two numbers not between a number and a string so that's why the type of the variables

251
00:24:21,120 --> 00:24:26,000
that you are passing to this operation were incorrect so let's take a look at the type of

252
00:24:26,000 --> 00:24:32,080
this particular column it says that it's an o type o stands for object over here what that means is

253
00:24:32,080 --> 00:24:38,640
that it's not a integer or it's not a float it's not a boolean it's a string type why that has

254
00:24:38,640 --> 00:24:43,920
happened is because the first row contained a string so that is why it casted everything

255
00:24:43,920 --> 00:24:49,120
else into a string as well it did not stop to interpret all of these things as numbers

256
00:24:50,160 --> 00:24:57,200
now if we say that duration is simply the call name from the first element onwards if you remember

257
00:24:57,200 --> 00:25:03,440
this is the syntax we use for slicing lists in python so when i say one colon what that means

258
00:25:03,440 --> 00:25:08,960
is that take everything from the first row onwards so let's try that and put that in a new variable

259
00:25:08,960 --> 00:25:15,360
called duration so when we take do ahead on duration you see that we have a bunch of things

260
00:25:15,360 --> 00:25:21,920
which are now numbers we have gotten rid of the very first column which was text right

261
00:25:21,920 --> 00:25:29,200
and now what we're going to do is use this function called pd.tume to numeric to convert

262
00:25:29,200 --> 00:25:36,400
this d type of object into a d type of integer let's try it we see this time we get it correctly

263
00:25:36,400 --> 00:25:43,200
it shows me the data but at the same time it's also telling me that the type is int64 what int64

264
00:25:43,200 --> 00:25:49,920
means is that each integer is being represented with 64 bits right so one way to do that is

265
00:25:49,920 --> 00:25:54,640
actually to apply a lambda function lambda functions in python are nothing but anonymous

266
00:25:54,640 --> 00:25:59,040
functions they are essentially functions that take a particular value and whatever you write

267
00:25:59,040 --> 00:26:03,840
next to it that is the value that they return so essentially it takes an x and it converts that x

268
00:26:03,840 --> 00:26:10,800
into an integer and returns it so what i do is i apply this particular function to each row of

269
00:26:10,800 --> 00:26:17,600
duration using the dot apply method so essentially duration here is my column and when i do a dot

270
00:26:17,600 --> 00:26:22,400
apply of this particular function what is going to do is simply take this function and apply it

271
00:26:22,400 --> 00:26:28,800
to every single row of that column and that output i'm capturing in a variable called duration in

272
00:26:28,800 --> 00:26:36,000
it which is my integer duration so there it is so again as we see when i do a head on the output

273
00:26:36,000 --> 00:26:42,560
column i get the time from start to finish in integers one easier way to do that is simply to

274
00:26:42,640 --> 00:26:48,080
duration dot as type of integer i can also do an as type of float but in this case we just

275
00:26:48,080 --> 00:26:52,640
we are just measuring seconds there is no concept of you know half second or something like that

276
00:26:52,640 --> 00:26:57,840
so integers is fine so let's take a look at this now now we have

277
00:27:01,600 --> 00:27:05,840
yeah that's basically the column we have which has a type of integer 64

278
00:27:05,840 --> 00:27:11,760
now for the first time in all of this exercise for the first time we have a column which is

279
00:27:11,760 --> 00:27:18,320
purely numerical and as you know what we do with numerical columns is do univariate analysis on them

280
00:27:18,320 --> 00:27:24,000
and one of the straightforward graphical methods of doing univariate analysis is to try and build

281
00:27:24,000 --> 00:27:31,920
a histogram so essentially let's do just that that is that's basically the histogram of our data

282
00:27:32,560 --> 00:27:38,640
that can be done simply by calling the dot hist function on any column in pandas so basically

283
00:27:38,640 --> 00:27:45,520
this was a column duration index and we just called it just called the hist function on it

284
00:27:45,520 --> 00:27:51,200
and we gave a parameter which is the number of bins that we want to divide the data into

285
00:27:51,200 --> 00:27:55,760
if you remember correctly the histogram what it does is that it takes the minimum value the maximum

286
00:27:55,760 --> 00:28:01,040
value and that range it divides equally into a bunch of bins and for each bins it's just going

287
00:28:01,040 --> 00:28:05,600
to count how many values lie within that bin so essentially we're saying that take the minimum

288
00:28:05,600 --> 00:28:11,920
value take the maximum value divide it into 50 equal bins and try to just count how many

289
00:28:13,440 --> 00:28:19,520
rows there are within each bin so as we can see from the histogram lots of values most of them

290
00:28:19,520 --> 00:28:24,400
in fact or 90 percent and more easily are concentrated in the very first one so which

291
00:28:24,400 --> 00:28:30,720
means that there are obviously a few outliers some people took a lot of time to a very small

292
00:28:30,720 --> 00:28:35,840
number of people but they did take a lot of time to fill the data set right so in order to make a

293
00:28:35,840 --> 00:28:40,160
little more sense of this particular histogram we are barely getting any information here all

294
00:28:40,160 --> 00:28:44,720
we're getting from this particular histogram is that most of the data is concentrated in the

295
00:28:44,720 --> 00:28:51,280
leftmost bin or the minimum bin of my data in order to see a little more clearer histogram what

296
00:28:51,280 --> 00:28:56,560
we're going to do is filter the data along this axis a little bit and look at the histogram when

297
00:28:56,560 --> 00:29:01,920
we have a smaller range of numbers so as we know we can do the filtering with Boolean indices

298
00:29:02,640 --> 00:29:08,160
if you remember how we filtered the name of Emily from the earlier example we had we added

299
00:29:08,160 --> 00:29:14,080
basically a logical condition like a double equal to operator and it returned a bunch of true and

300
00:29:14,080 --> 00:29:19,040
false values and we use those true and false values to pick rows from the data set right and

301
00:29:19,040 --> 00:29:25,360
that's exactly what I'm going to do over here we say that find those rows in duration and ends such

302
00:29:25,360 --> 00:29:30,320
that their value is less than 60 so we are basically trying to find out all the people who

303
00:29:30,960 --> 00:29:36,240
submitted or finished the survey in less than 60 seconds and from that we are going to calculate

304
00:29:36,240 --> 00:29:44,160
the histogram so let's take a look at this so there it is we see that about a little less than

305
00:29:44,160 --> 00:29:49,680
10 people finished it in 25 seconds and about 50 or so people finished it in less than 60 seconds

306
00:29:50,400 --> 00:29:54,960
now the thing is that all of these people probably didn't take the survey very seriously so this is

307
00:29:54,960 --> 00:30:00,160
you know this is some data we might have to drop because they probably just randomly clipped

308
00:30:00,160 --> 00:30:05,920
something and in general there since they finished such a complicated survey very quickly

309
00:30:06,480 --> 00:30:13,920
their input is not very reliable so essentially now we are going to use these filters to filter

310
00:30:13,920 --> 00:30:19,920
out the entire data set so what we do is we create a new column called to drop which is simply going

311
00:30:19,920 --> 00:30:25,920
to contain the index values of all those peoples whose duration was less than 60 seconds so let's

312
00:30:25,920 --> 00:30:32,000
do that and when we look at the to drop function created thus we see that row number 100 and 451

313
00:30:32,000 --> 00:30:43,920
700 7844 10 526 all the way up to 1 919 706 these are basically the row positions

314
00:30:43,920 --> 00:30:50,320
whose duration was less than 60 seconds so if we go back to the data we see that we have these

315
00:30:50,320 --> 00:31:00,000
numbers 0 1 2 3 as our row labels now these are the row labels such that their duration in

316
00:31:00,000 --> 00:31:05,440
integers was less than 60 and these are the ones that we have to drop in order to do that we use

317
00:31:05,440 --> 00:31:11,040
the drop method which is also available on the data frame itself so what i say is df.drop and

318
00:31:11,040 --> 00:31:15,760
then i give it the values to drop which is essentially the list of these values and i say

319
00:31:15,760 --> 00:31:21,840
that drop them along the 0th axis the 0th axis basically means that i want to drop them along

320
00:31:21,840 --> 00:31:28,160
the rows not the columns so essentially the 0 axis is since it's a two-dimensional data frame it has

321
00:31:28,160 --> 00:31:33,520
two axis and since counting starts from zero the 0th axis is the rows and the first axis is the

322
00:31:33,520 --> 00:31:38,560
columns so we want to drop along this along this particular axis those values don't exist at all

323
00:31:38,640 --> 00:31:45,440
it's the column headers that exist right so we can think of this as a little bit of data sanitization

324
00:31:45,440 --> 00:31:50,400
and we want to do it in place we want to modify the existing data frame to drop these values we

325
00:31:50,400 --> 00:31:55,040
don't want to create a new data frame right so we just add that in place equal to true flag over

326
00:31:55,040 --> 00:32:00,400
here which is just going to drop all of that data and then when we do df.head we see that we have

327
00:32:00,400 --> 00:32:04,480
all the values that are greater than 60 in fact when we try to do

328
00:32:04,480 --> 00:32:14,320
first of all if we get the call name we get the data again and then when i try to do an iloc of

329
00:32:14,320 --> 00:32:19,040
one onwards which is all of these things without worrying about this particular

330
00:32:22,640 --> 00:32:26,720
value which is actually a text and therefore might prevent me from calculating minimums

331
00:32:26,720 --> 00:32:32,480
and maximums then i'm going to convert it into integers and then i'm going to take a minimum

332
00:32:32,480 --> 00:32:38,400
value and as we see the minimum value is 60 which means that we are only keeping those people in our

333
00:32:40,640 --> 00:32:47,760
data who have submitted who have taken at least one minute to submit the survey now of course this

334
00:32:47,760 --> 00:32:56,720
choice of 60 seconds is arbitrary you can easily say that you know how do we know that somebody who

335
00:32:56,720 --> 00:33:02,560
did not finish it in one second or one minute one second was actually not you know was taking

336
00:33:02,560 --> 00:33:08,080
it seriously so this is a completely arbitrary value you can choose your own the idea is just

337
00:33:08,080 --> 00:33:13,840
to show you how we can do filtering and use that filtering to drop particular rows which you think

338
00:33:13,840 --> 00:33:22,640
might be unclean now we're going to continue with this data set in the next notebook and in order to

339
00:33:22,640 --> 00:33:26,880
do that we are going to save the clean data that we have they're just going calling it clean because

340
00:33:26,880 --> 00:33:32,720
we have dropped some rows which you know by some definition we have designated as dirty or suspicious

341
00:33:32,720 --> 00:33:39,040
suspicious rows in order to save a file or a data frame to a file just like we did a read csv

342
00:33:39,040 --> 00:33:44,240
there is also a two csv you just give the path to the file that you want and you say that index is

343
00:33:44,240 --> 00:33:49,120
false which simply means that this particular column which is our index column we don't want

344
00:33:49,120 --> 00:33:52,320
to write it back because it's just zero one two three there's no point writing it back

345
00:33:52,960 --> 00:33:58,960
it's just going to infer it back from the data when we read it again so let's write that data

346
00:33:58,960 --> 00:34:05,440
now our data is written and make sure that you actually finish this step properly because we are

347
00:34:05,440 --> 00:34:11,280
going to depend on this clean data in the next video when we do more mathematical and more

348
00:34:11,280 --> 00:34:16,480
statistical analysis on this data set

