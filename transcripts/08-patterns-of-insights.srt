1
00:00:00,000 --> 00:00:02,760
Welcome back.

2
00:00:02,760 --> 00:00:07,120
By now, we know how to do a significant amount of data analysis.

3
00:00:07,120 --> 00:00:13,400
Specifically, we know the mechanics of performing data analysis on arbitrary datasets.

4
00:00:13,400 --> 00:00:18,440
The purpose of this video is to develop a strategy for how we can frame this analysis

5
00:00:18,440 --> 00:00:25,720
into individual insights and then into a structured storyline that we can finally communicate.

6
00:00:25,720 --> 00:00:32,040
This is important because as we have seen, analysis by itself is fairly mechanical and

7
00:00:32,040 --> 00:00:35,240
it generates just facts and figures.

8
00:00:35,240 --> 00:00:41,660
More specifically, we are now equipped with the tools required to build dashboards.

9
00:00:41,660 --> 00:00:45,080
But remember that dashboards have their own problems.

10
00:00:45,080 --> 00:00:49,840
You might remember this slide from one of our previous videos and we have gone over

11
00:00:49,840 --> 00:00:52,800
how dashboards leave some things unclarified.

12
00:00:52,800 --> 00:00:56,720
Specifically, what question does the dashboard answer?

13
00:00:56,720 --> 00:01:02,040
Is the answer evident from the dashboard and what action should the user take?

14
00:01:02,040 --> 00:01:06,880
If we know how to perform analysis, then we know what to put in a dashboard.

15
00:01:06,880 --> 00:01:09,140
But that itself is not enough.

16
00:01:09,140 --> 00:01:14,080
The purpose of this video is to come up with a strategy which enables us to bridge that

17
00:01:14,080 --> 00:01:19,640
gap going from having data analytics ready with us to actually answering questions at

18
00:01:19,640 --> 00:01:23,760
the level of the business problems.

19
00:01:23,760 --> 00:01:27,940
You can look at this problem in a different context as well.

20
00:01:27,940 --> 00:01:32,480
As you remember, this is the life of data cycle that we had proposed at the beginning

21
00:01:32,480 --> 00:01:33,920
of this course.

22
00:01:33,920 --> 00:01:40,160
And we said that we would be focusing on the last two stages here, that is analysis and

23
00:01:40,160 --> 00:01:41,360
consumption.

24
00:01:41,360 --> 00:01:46,220
By now, we know how to do the third step, analysis.

25
00:01:46,220 --> 00:01:51,460
The focus of the upcoming material is on the last part, that is the consumption of

26
00:01:51,460 --> 00:01:52,460
analytics.

27
00:01:52,460 --> 00:01:59,320
To reiterate, because performing analysis is a very algorithmic mechanical process,

28
00:01:59,320 --> 00:02:03,660
it can be automated and that's exactly what dashboards do.

29
00:02:03,660 --> 00:02:08,620
But because it is often left unsupervised, most dashboards just end up throwing a lot

30
00:02:08,620 --> 00:02:13,540
of information on the screen without any real insight or hints as to how to navigate the

31
00:02:13,540 --> 00:02:19,900
dashboard, how to drill down on a particular element to go deeper into the analysis, etc.

32
00:02:19,900 --> 00:02:25,900
So essentially the consumer is helpless until they find someone who is capable of reading

33
00:02:25,900 --> 00:02:28,560
and navigating a particular dashboard.

34
00:02:28,560 --> 00:02:33,660
This entire scenario is what we want to avoid.

35
00:02:33,660 --> 00:02:39,500
Specifically as business analysts, we want to create meaningful insights.

36
00:02:39,500 --> 00:02:46,140
We don't want to overwhelm our users with analytical details, but we want to be straightforward

37
00:02:46,140 --> 00:02:47,980
with our findings instead.

38
00:02:47,980 --> 00:02:51,640
Secondly, we want to present our work effectively.

39
00:02:51,640 --> 00:02:57,020
The user should not feel like they are navigating a maze when they are consuming our work.

40
00:02:57,020 --> 00:03:00,780
And finally, we want to make our work easy to consume.

41
00:03:00,780 --> 00:03:08,920
Users should be able to at a glance find enough information which enables them to make a decision.

42
00:03:08,920 --> 00:03:12,280
As an example, look at this infographic.

43
00:03:12,280 --> 00:03:17,040
This is something we built for one of our clients, a major pharmaceutical company.

44
00:03:17,040 --> 00:03:22,640
I'd like you to pause the video right now, look at this in detail for a few minutes and

45
00:03:22,640 --> 00:03:25,160
try to understand what this actually says.

46
00:03:25,160 --> 00:03:29,280
I'll explain the answer shortly, but it might be useful to see how much of this you can

47
00:03:29,280 --> 00:03:38,040
understand all by yourself just by reading this slide.

48
00:03:38,040 --> 00:03:41,520
So here is what this infographic is about.

49
00:03:41,520 --> 00:03:48,800
It describes the flow of 2054 service requests that were lodged with this company and how

50
00:03:48,800 --> 00:03:53,160
they journey through the redressal mechanism of this organization.

51
00:03:53,160 --> 00:03:57,520
Each bubble here represents a stage of the flow.

52
00:03:57,520 --> 00:04:02,800
So you have a stage where the request is drafted, then it is submitted, then it goes to waiting

53
00:04:02,800 --> 00:04:08,080
approval from which it might be rejected or it could go to the planning stage.

54
00:04:08,080 --> 00:04:10,040
After that, it enters the pending stage.

55
00:04:10,040 --> 00:04:14,160
After that, it might go to cancel stage or it could go to the in-progress stage.

56
00:04:14,160 --> 00:04:19,320
After that, it goes to completion and then finally to the closed state.

57
00:04:19,320 --> 00:04:25,120
The size of each bubble represents how many services are stuck at that particular stage.

58
00:04:25,120 --> 00:04:32,640
So we see that the three biggest examples we have here is that 31% of service requests

59
00:04:32,640 --> 00:04:41,080
are stuck in the in-progress phase, 17.6% are waiting approval and 19.6% have been completed.

60
00:04:41,080 --> 00:04:45,520
And finally, the color represents whether the bulk of the requests are likely to proceed

61
00:04:45,520 --> 00:04:46,520
or receive.

62
00:04:46,520 --> 00:04:52,440
That is, are they more likely to go towards the completion or are they more likely to

63
00:04:52,440 --> 00:04:53,440
go back?

64
00:04:53,560 --> 00:04:59,680
A green bubble and a green path, so something like this or something like this, means that

65
00:04:59,680 --> 00:05:03,640
requests are more likely to go forward, that is towards completion.

66
00:05:03,640 --> 00:05:10,600
A red bubble or a red path like this one means that requests are likely to go back in the

67
00:05:10,600 --> 00:05:11,600
flow.

68
00:05:11,600 --> 00:05:16,240
And it is here that we see our first big anomaly.

69
00:05:16,240 --> 00:05:19,080
Why is the completed bubble red?

70
00:05:19,720 --> 00:05:24,200
Actually, if a request is completed, it should go straight away to closed and the bubble

71
00:05:24,200 --> 00:05:26,240
should be green, it should not be red.

72
00:05:26,240 --> 00:05:31,960
But it seems like most of the completed requests are going all the way back to the submitted

73
00:05:31,960 --> 00:05:32,960
stage.

74
00:05:32,960 --> 00:05:33,960
That's really strange.

75
00:05:33,960 --> 00:05:38,120
Now, here is the bigger point in this discussion.

76
00:05:38,120 --> 00:05:42,560
All of this explanation which I have given you about reading this infographic is very

77
00:05:42,560 --> 00:05:48,280
difficult to infer just from reading the chart on your own and our client probably might

78
00:05:48,280 --> 00:05:50,560
have felt the same at some point.

79
00:05:50,560 --> 00:05:54,840
There is no way that anyone could have inferred all of these things at a glance by simply

80
00:05:54,840 --> 00:05:57,720
looking at this dashboard.

81
00:05:57,720 --> 00:06:02,920
Which means that the user has to depend either on us or on some other data analyst to just

82
00:06:02,920 --> 00:06:04,800
interpret charts.

83
00:06:04,800 --> 00:06:07,880
That's a pure and simple waste of time and money.

84
00:06:07,880 --> 00:06:12,120
So leaving things up to interpretation is not always a good idea.

85
00:06:12,120 --> 00:06:16,960
Then again, even after I've explained all of this, the user is likely to say that, boss,

86
00:06:17,000 --> 00:06:18,440
I already know all of this.

87
00:06:18,440 --> 00:06:24,440
I know already that most of my services are stuck at the in progress stage.

88
00:06:24,440 --> 00:06:30,880
I know already that most of my requests go from completion to submission.

89
00:06:30,880 --> 00:06:32,320
After all, this is my data.

90
00:06:32,320 --> 00:06:35,320
What you have to figure out is why is this happening?

91
00:06:35,320 --> 00:06:40,360
So then we went into the root cause analysis and we tried to figure out why these anomalies

92
00:06:40,360 --> 00:06:41,360
are happening.

93
00:06:41,560 --> 00:06:47,360
And as a result of that, we added this executive summary over here, which the user can just

94
00:06:47,360 --> 00:06:48,360
read.

95
00:06:48,360 --> 00:06:53,280
The chart at this point simply becomes proof of what we are trying to convey.

96
00:06:53,280 --> 00:06:57,920
It does not become the central point of focus.

97
00:06:57,920 --> 00:07:02,880
One of the most important reasons why this happens is because we tend to focus more on

98
00:07:02,880 --> 00:07:05,440
analysis than on insight.

99
00:07:05,440 --> 00:07:07,360
So here is the big idea.

100
00:07:07,360 --> 00:07:10,220
Analysis is not insights.

101
00:07:10,220 --> 00:07:12,460
Analysis is plain bare facts.

102
00:07:12,460 --> 00:07:17,380
Yes, some of them may not be apparent and they are definitely important, but at the

103
00:07:17,380 --> 00:07:22,300
same time, once they are found, they may have limited value.

104
00:07:22,300 --> 00:07:24,540
Insights on the other hand are actionable.

105
00:07:24,540 --> 00:07:27,740
They empower our users to take action.

106
00:07:27,740 --> 00:07:31,140
So generally what we need to do is apply filters to our analysis.

107
00:07:31,140 --> 00:07:37,100
Out of the entire set of analyses that we may have done on a particular data set, we

108
00:07:37,100 --> 00:07:42,740
need to figure out which of them actually contribute to insights and which of them are

109
00:07:42,740 --> 00:07:49,700
just pure information, which the user may already know in many cases.

110
00:07:49,700 --> 00:07:51,540
And that is the problem with dashboards.

111
00:07:51,540 --> 00:07:54,860
They tend to give us analysis, not insights.

112
00:07:54,860 --> 00:08:00,060
Let's take a moment to think about what we do when we build dashboards.

113
00:08:00,060 --> 00:08:04,820
We have some data, we have some analytics on the data, we do a bunch of fairly routine

114
00:08:04,820 --> 00:08:09,620
and maybe some domain specific operations on the data and then something clicks.

115
00:08:09,620 --> 00:08:15,260
I get an insight, I get a nugget of wisdom in the data and then I put a widget in my

116
00:08:15,260 --> 00:08:22,140
dashboard, either a chart or a piece of text or some graphics, whatever, which reflects

117
00:08:22,140 --> 00:08:23,140
that insight.

118
00:08:23,140 --> 00:08:29,300
Now, the problem is that at the initial stage, we have this data set, which can be fairly

119
00:08:29,300 --> 00:08:30,300
ambiguous.

120
00:08:30,300 --> 00:08:32,180
We don't yet know what it means.

121
00:08:32,180 --> 00:08:36,340
So we perform some analysis on it, which reduces its complexity a little bit.

122
00:08:36,340 --> 00:08:41,300
We get the insight, which by definition is less complex and more meaningful than the

123
00:08:41,300 --> 00:08:42,300
raw data.

124
00:08:42,300 --> 00:08:47,660
And then in my spreadsheet or my dashboard, I am again increasing its complexity by communicating

125
00:08:47,660 --> 00:08:51,780
that insight through a chart that cannot be avoided, of course.

126
00:08:51,780 --> 00:08:57,180
But the fact remains that it is somewhat counterproductive and we tend to get carried away with this

127
00:08:57,180 --> 00:08:58,180
many times.

128
00:08:58,180 --> 00:09:01,260
No analyst will ever want to hide their work.

129
00:09:01,340 --> 00:09:06,500
We usually go to great lengths to describe in detail what we've done, but not enough

130
00:09:06,500 --> 00:09:09,700
attention is given to what we have found.

131
00:09:09,700 --> 00:09:13,460
So you see, an Excel spreadsheet is not a dashboard.

132
00:09:13,460 --> 00:09:15,740
The spreadsheet is where you do the analysis.

133
00:09:15,740 --> 00:09:20,100
It has to be distilled into something the users can consume without going through the

134
00:09:20,100 --> 00:09:23,020
same journey that the analyst has themselves.

135
00:09:23,020 --> 00:09:29,140
Therefore, a dashboard has to be simpler than the data it explains.

136
00:09:29,140 --> 00:09:34,100
It can't just throw data in your face and it should have just enough flexibility that

137
00:09:34,100 --> 00:09:39,180
users can replicate the insights, not the analysis itself.

138
00:09:39,180 --> 00:09:44,940
And of course, it has to lean more towards prescriptive rather than descriptive analytics.

139
00:09:44,940 --> 00:09:49,300
The user should be able to take away something from the results, which is not simply facts

140
00:09:49,300 --> 00:09:51,980
and figures.

141
00:09:51,980 --> 00:09:58,260
So this brings us to the question, what separates insights from analysis to enable us to perform

142
00:09:58,260 --> 00:10:04,540
this filtering insights from analysis, we propose a framework called Patterns of Insights.

143
00:10:04,540 --> 00:10:08,980
This framework will allow us to figure out if some particular piece of analysis counts

144
00:10:08,980 --> 00:10:12,780
as a meaningful insight or not.

145
00:10:12,780 --> 00:10:20,020
So there are five basic patterns of insights, unknown result, surprising comparison, surprising

146
00:10:20,020 --> 00:10:24,900
extremes, significant outliers and abnormal distributions.

147
00:10:24,900 --> 00:10:30,140
Almost all insights fall into one of these patterns and knowing them beforehand helps

148
00:10:30,140 --> 00:10:31,940
us frame the right question.

149
00:10:31,940 --> 00:10:39,540
So let's go through these one by one.

150
00:10:39,540 --> 00:10:42,100
The first type of insight is an unknown result.

151
00:10:42,100 --> 00:10:45,860
For example, the national animal of Scotland is a unicorn.

152
00:10:45,860 --> 00:10:52,180
I did not know this and it doesn't even make sense because a unicorn is not really an animal,

153
00:10:52,180 --> 00:10:53,180
but it's a fact.

154
00:10:53,300 --> 00:10:58,700
However, we are more likely to figure out results of the following form, which is revenue

155
00:10:58,700 --> 00:11:03,740
of a particular company has increased by X percent from the last quarter or sales have

156
00:11:03,740 --> 00:11:07,580
decreased by Y percent in this particular financial year.

157
00:11:07,580 --> 00:11:12,820
So these are the most common type of insights that we are likely to come up with.

158
00:11:12,820 --> 00:11:17,460
We do some computation and we just report the result, but is the result really unknown?

159
00:11:17,460 --> 00:11:21,620
That is not obvious when you're doing your analysis, but you can always ask your audience.

160
00:11:21,620 --> 00:11:25,220
You can always ask, hey, did you know that X is happening?

161
00:11:25,220 --> 00:11:29,820
Now they may say yes, they may say no, but as you get more and more experience in a given

162
00:11:29,820 --> 00:11:34,500
domain, as you do more and more practice, it will become easier to determine whether

163
00:11:34,500 --> 00:11:40,540
a particular fact that you have derived from your data is known or unknown.

164
00:11:40,540 --> 00:11:43,980
The second is a surprising comparison.

165
00:11:43,980 --> 00:11:49,140
We just take any metric and compare it with the corresponding metric of some other cohort.

166
00:11:49,140 --> 00:11:54,340
Now that cohort could be your own figures from a different time frame or that of a competitor

167
00:11:54,340 --> 00:11:57,220
or anything else that you want to compare against.

168
00:11:57,220 --> 00:12:00,500
If it is surprising enough, then it's a surprising comparison.

169
00:12:00,500 --> 00:12:08,620
Now whether the comparison is surprising enough or not is a largely subjective question, but

170
00:12:08,620 --> 00:12:13,620
we can deal with this subjectiveness again with some practice and experience.

171
00:12:13,620 --> 00:12:18,420
A couple of examples are here, we say that our revenue has doubled since last quarter.

172
00:12:18,420 --> 00:12:22,780
So here we have a particular comparison, this time with our own data in a previous

173
00:12:22,780 --> 00:12:23,860
data frame.

174
00:12:23,860 --> 00:12:27,780
We can also say that sales are only half of that of our competitor.

175
00:12:27,780 --> 00:12:32,300
Here we are comparing a particular metric with a different cohort, that cohort being

176
00:12:32,300 --> 00:12:37,940
the data set which belongs to a particular competitor, for example.

177
00:12:37,940 --> 00:12:42,460
The next pattern of insight is surprising extremes.

178
00:12:42,460 --> 00:12:46,300
It basically answers questions like what is the highest, what is the lowest?

179
00:12:46,300 --> 00:12:50,300
Take any metric in your data set and find its lowest and highest values.

180
00:12:50,300 --> 00:12:54,260
If the corresponding details are surprising, then you have found your insight.

181
00:12:54,260 --> 00:12:59,700
For example, if you look at LPG coverage in India, you will find that it's the lowest

182
00:12:59,700 --> 00:13:02,240
in some of the poorest states.

183
00:13:02,240 --> 00:13:08,460
Now it's understandable that Andaman and Nicobar Islands might have a low coverage because

184
00:13:08,460 --> 00:13:12,780
the islands are far away from the mainland and there may be logistical issues involved

185
00:13:12,780 --> 00:13:16,740
in getting gas cylinders all the way to the island.

186
00:13:16,740 --> 00:13:22,580
But it's surprising that it's so low in Bihar, Jharkhand and Orissa, given that the Pradhan

187
00:13:22,580 --> 00:13:29,060
Mantri Ujwala Ayojna is targeted towards providing free LPG to women who are below the poverty

188
00:13:29,060 --> 00:13:30,060
line.

189
00:13:30,060 --> 00:13:32,980
And by the way, this is all from the government's own data.

190
00:13:32,980 --> 00:13:36,300
So there's definitely something surprising going on here.

191
00:13:36,300 --> 00:13:40,540
To summarize, you can ask any question which contains extremes.

192
00:13:40,540 --> 00:13:42,420
What is the highest performing product?

193
00:13:42,420 --> 00:13:44,180
Which player scored the most?

194
00:13:44,180 --> 00:13:46,500
Which demographic has the least average income?

195
00:13:46,500 --> 00:13:47,500
Anything of this sort.

196
00:13:47,500 --> 00:13:53,220
And if the result is surprising, then you have found your insight.

197
00:13:53,220 --> 00:13:58,780
The next pattern is significant outliers, which is a special case of surprising extremes

198
00:13:58,780 --> 00:14:00,460
in the last pattern.

199
00:14:00,460 --> 00:14:04,460
Remember that for any metric, extremes will always exist.

200
00:14:04,460 --> 00:14:07,920
There will always be a highest and a lowest value.

201
00:14:07,920 --> 00:14:13,560
The question is, is this value so higher or lower than the next highest or lowest value

202
00:14:13,560 --> 00:14:15,360
that it counts as an outlier?

203
00:14:15,360 --> 00:14:20,160
So let's say if I ask something like, who's the tallest person in the room?

204
00:14:20,160 --> 00:14:21,160
And somebody stands up.

205
00:14:21,160 --> 00:14:25,600
And if we do realize that, OK, that tallest person is actually seven or eight feet tall,

206
00:14:25,600 --> 00:14:28,840
then that's a surprising, that's a significant outlier.

207
00:14:28,840 --> 00:14:33,640
If that is the case, then we know that is how you can detect significant outliers.

208
00:14:33,640 --> 00:14:41,360
For example, if we say that startups in Bangalore have 10 times greater chance of getting funded

209
00:14:41,360 --> 00:14:43,200
than startups in any other place.

210
00:14:43,200 --> 00:14:49,480
Now, obviously, we know that if a startup belongs in a city like Bangalore, which is

211
00:14:49,480 --> 00:14:55,640
known for its startup ecosystem, then they would have a high chance of getting funded.

212
00:14:55,640 --> 00:15:01,080
But that chance is actually 10 times higher than any other city in the country, then that's

213
00:15:01,080 --> 00:15:03,260
a significant outlier.

214
00:15:03,260 --> 00:15:08,340
If you remember, this is some analysis we did on the Kaggle data science survey.

215
00:15:08,340 --> 00:15:14,820
And we see that out of all the people, especially Indians, who are present on Kaggle, the most,

216
00:15:14,820 --> 00:15:20,380
the largest job designations are those of data scientists and software engineers, which

217
00:15:20,380 --> 00:15:21,380
makes sense.

218
00:15:21,380 --> 00:15:25,420
Software engineers and data scientists are quite likely to end up very frequently on

219
00:15:25,420 --> 00:15:26,420
Kaggle.

220
00:15:26,420 --> 00:15:32,180
But the fact that they are more than twice the size of data analysts available, that

221
00:15:32,180 --> 00:15:33,180
is a surprising outlier.

222
00:15:33,180 --> 00:15:38,540
We would have expected data analysts to be a little less than data scientists, but not

223
00:15:38,540 --> 00:15:41,620
so less, not less than half.

224
00:15:41,620 --> 00:15:48,480
Finally, the last pattern of insight is what we call an abnormal distribution.

225
00:15:48,480 --> 00:15:51,740
These are mostly just unusual patterns.

226
00:15:51,740 --> 00:15:56,140
For example, whenever you are measuring something, you might expect it to follow a specific kind

227
00:15:56,140 --> 00:15:57,620
of distribution.

228
00:15:57,620 --> 00:16:01,540
But if it has anomalies, then it might be worth investigating.

229
00:16:01,540 --> 00:16:07,020
For example, here is a heat map that denotes how many births have taken place in India

230
00:16:07,020 --> 00:16:13,500
on average on each day of the year, from 2017 to 2007 to 2013.

231
00:16:13,500 --> 00:16:18,860
A darker color indicates that more children were born on that day.

232
00:16:18,860 --> 00:16:25,940
Now, we would expect birth to be completely random, but we see that there are clearly

233
00:16:25,940 --> 00:16:28,300
some significant patterns here.

234
00:16:28,300 --> 00:16:32,460
Like nobody, almost nobody seems to be born in August.

235
00:16:32,460 --> 00:16:34,580
Now, there's a reason for this.

236
00:16:34,580 --> 00:16:36,300
It's actually because of school admissions.

237
00:16:36,300 --> 00:16:41,380
If you are actually born in August, then you're just exactly a month old, month too old to

238
00:16:41,380 --> 00:16:43,260
join school at the right time.

239
00:16:43,260 --> 00:16:49,940
So what your parents will do is try to squeeze you into the correct academic year by reducing

240
00:16:49,940 --> 00:16:53,580
your date of birth by a month or two months.

241
00:16:53,580 --> 00:16:56,820
So most parents will fake the date of the birth.

242
00:16:56,820 --> 00:17:05,460
Also, we see that we have darker patterns on the 5th, the 10th, the 15th, the 20th,

243
00:17:05,460 --> 00:17:08,580
the 25th, and the 30th.

244
00:17:08,580 --> 00:17:12,540
This is because when people try to come up with fake numbers, they usually come up with

245
00:17:12,540 --> 00:17:13,740
round numbers.

246
00:17:13,740 --> 00:17:17,700
If you want to make up your child's date of birth, it's much easier to say he was born

247
00:17:17,700 --> 00:17:22,980
on the 10th of June or the 15th of July, not something like the 13th of June.

248
00:17:22,980 --> 00:17:26,780
So this is an example of an abnormal distribution.

249
00:17:26,780 --> 00:17:33,260
In this case, we expected randomness, but we found some manmade patterns.

250
00:17:33,260 --> 00:17:35,700
It can actually be the other way around as well.

251
00:17:35,700 --> 00:17:39,220
You could expect to see patterns, but you end up seeing randomness.

252
00:17:39,220 --> 00:17:44,540
That is also an abnormal distribution, essentially.

253
00:17:44,540 --> 00:17:49,700
So broadly, these are the five patterns of insights, which we can use to categorize our

254
00:17:50,700 --> 00:17:55,180
For every piece of analytical fact that we derive, we can ask, does it fall into one

255
00:17:55,180 --> 00:17:56,540
of these categories?

256
00:17:56,540 --> 00:18:03,340
If it does, then it becomes a good candidate for our final report.

257
00:18:03,340 --> 00:18:08,780
But note that when we are doing the sort of analysis and categorization on a given problem,

258
00:18:08,780 --> 00:18:11,500
we do not do it on the raw data alone.

259
00:18:11,500 --> 00:18:15,340
We do it on the raw data, but we also augment the data set.

260
00:18:15,340 --> 00:18:20,060
These augmentations, again, fall into two broad categories.

261
00:18:20,060 --> 00:18:25,260
One is the augmentation of data by deriving the columns, and the other is augmentation

262
00:18:25,260 --> 00:18:27,440
by summarizing the data itself.

263
00:18:27,440 --> 00:18:30,980
This adds columns, whereas this shrinks rows.

264
00:18:30,980 --> 00:18:32,900
That might be useful to remember.

265
00:18:32,900 --> 00:18:38,900
When we talk of deriving columns, we are essentially adding more columns to the data set by deriving

266
00:18:38,900 --> 00:18:42,980
them from existing columns or other data sets as well.

267
00:18:43,500 --> 00:18:49,820
For example, let us say that we have the mortality data for COVID-19 for every state in India.

268
00:18:49,820 --> 00:18:55,140
Then for each state, we can look up more data from whatever resources are available publicly.

269
00:18:55,140 --> 00:19:00,100
For example, if we can get the health budget for each state, we can ask a simple question.

270
00:19:00,100 --> 00:19:04,980
For states that are spending more on health care, are the COVID fatalities lower?

271
00:19:04,980 --> 00:19:07,660
We can also do the same with sanitation data.

272
00:19:07,660 --> 00:19:12,340
So essentially, given a column, we can see if we can look up other data from other data

273
00:19:12,340 --> 00:19:16,300
sets, which helps us augment our analysis.

274
00:19:16,300 --> 00:19:17,300
We can also transform data.

275
00:19:17,300 --> 00:19:23,460
If we have something like age, we can bin that or transform that particular column to

276
00:19:23,460 --> 00:19:27,580
divide people into buckets like young, middle-aged, and old.

277
00:19:27,580 --> 00:19:33,880
We can transform socioeconomic factors like income and education into categorical variables

278
00:19:33,880 --> 00:19:40,360
like poor versus rich or educated versus uneducated and so on.

279
00:19:40,360 --> 00:19:45,680
One other way in which we can add more columns is by using machine learning.

280
00:19:45,680 --> 00:19:50,720
We can use modern predictions to do some sort of clustering on say customer data and try

281
00:19:50,720 --> 00:19:54,240
to figure out which of them have similar characteristics.

282
00:19:54,240 --> 00:19:57,880
So we can sort of profile our customers using machine learning.

283
00:19:57,880 --> 00:20:02,680
Suppose we are working from Amazon or any other e-commerce company and we want to figure

284
00:20:02,680 --> 00:20:06,600
out which products should we offer discounts upon.

285
00:20:06,600 --> 00:20:12,000
We can look at the customer reviews and perform say something like sentiment analysis on the

286
00:20:12,000 --> 00:20:13,000
reviews.

287
00:20:13,000 --> 00:20:17,320
The sentiment then becomes a new column and we can say we can try focusing our analysis

288
00:20:17,320 --> 00:20:20,400
by using the sentiment as the pivotal column.

289
00:20:20,400 --> 00:20:25,920
So for those products that have bad reviews, we might want to offer a different rate of

290
00:20:25,920 --> 00:20:26,920
discount.

291
00:20:26,920 --> 00:20:31,600
Whereas for those products which have good reviews, we might want to offer a different

292
00:20:31,600 --> 00:20:32,600
rate of discount.

293
00:20:32,600 --> 00:20:37,120
We could also try and see what exactly is causing those bad reviews or good reviews.

294
00:20:37,120 --> 00:20:38,120
Is it the vendor?

295
00:20:38,120 --> 00:20:39,320
Is it the location?

296
00:20:39,320 --> 00:20:40,880
Is it the quality of the product?

297
00:20:40,880 --> 00:20:42,640
Whatever it could be.

298
00:20:42,640 --> 00:20:46,520
So all of this falls under adding new columns to the data set.

299
00:20:46,520 --> 00:20:50,160
The other way of augmentation is summarization.

300
00:20:50,160 --> 00:20:55,560
This is usually easier and we have already covered a bunch of these techniques already.

301
00:20:55,560 --> 00:21:01,600
So with all of these methods, we essentially have more columns and more metrics and then

302
00:21:01,600 --> 00:21:05,720
we can reapply the patterns of insights on this augmented data set.

303
00:21:05,720 --> 00:21:09,920
That just gives us a lot of new things to work with.

304
00:21:09,920 --> 00:21:15,400
But even with all of these checks and balances, it is possible to end up with a set of insights,

305
00:21:15,400 --> 00:21:21,360
not all of which we might want to include in our final presentation or our report.

306
00:21:21,360 --> 00:21:26,520
Note that these are all insights that are meaningful from the analyst's point of view.

307
00:21:26,520 --> 00:21:29,080
They may or may not be useful to the consumer.

308
00:21:29,080 --> 00:21:33,680
So we need to take a slightly more client centric approach.

309
00:21:33,680 --> 00:21:40,200
Well, the second advantage of following the patterns of insights is that they allow us

310
00:21:40,200 --> 00:21:41,200
to frame questions.

311
00:21:41,200 --> 00:21:46,480
In a sense, they allow us to reverse engineer our analysis to some extent.

312
00:21:46,480 --> 00:21:51,240
What we would normally do is perform exhaustive analysis on the whole data set and then put

313
00:21:51,240 --> 00:21:57,080
all the analysis through a funnel and filter out insights based on these patterns of insights.

314
00:21:57,080 --> 00:22:03,040
On the other hand, what we can do is we frame questions up front using these patterns before

315
00:22:03,040 --> 00:22:04,600
doing the analysis.

316
00:22:04,600 --> 00:22:07,240
And then the analysis just has to answer these questions.

317
00:22:07,240 --> 00:22:12,580
So that restricts our work a bit and it gives it a little more structure.

318
00:22:12,580 --> 00:22:16,160
So let's see how to do that.

319
00:22:16,160 --> 00:22:19,520
For each pattern, we can create a template for questions.

320
00:22:19,520 --> 00:22:23,920
And depending on the data set, we can use that template to sort of fill in the blanks

321
00:22:23,920 --> 00:22:25,800
and create the question.

322
00:22:25,800 --> 00:22:32,880
So for example, in the case of our first pattern, which is unknown result, this is the template.

323
00:22:32,880 --> 00:22:35,120
What is the metric for a given value?

324
00:22:35,120 --> 00:22:39,080
The metric can be any metric and the value can be any column in the data set.

325
00:22:39,080 --> 00:22:44,400
This sounds very general, simply like asking what is x and what is y, but then this is

326
00:22:44,400 --> 00:22:46,220
the most common pattern of insight.

327
00:22:46,220 --> 00:22:47,920
So that's not surprising.

328
00:22:47,920 --> 00:22:50,680
And we can use this template to come up with questions.

329
00:22:50,680 --> 00:22:55,240
For example, what is the average rainfall in Uganda?

330
00:22:55,240 --> 00:23:02,880
Here the metric is average, the value is rainfall, and Uganda is some filter that you have applied

331
00:23:02,880 --> 00:23:05,560
on a country column, for example.

332
00:23:05,560 --> 00:23:11,480
Note that you don't have to use these templates as literally as fill in the blanks kind of

333
00:23:11,480 --> 00:23:12,480
exercises.

334
00:23:12,480 --> 00:23:13,480
They are more like pointers.

335
00:23:13,480 --> 00:23:14,480
They are guidelines.

336
00:23:14,480 --> 00:23:16,800
Consider the second question.

337
00:23:16,800 --> 00:23:19,520
What is the cheapest place to buy coffee in Mumbai?

338
00:23:19,520 --> 00:23:24,760
Here the value is the price of coffee and the metric is minimization, which we have

339
00:23:24,760 --> 00:23:26,960
dubbed as cheapest.

340
00:23:26,960 --> 00:23:31,160
So the question doesn't look like it came from this particular template, but it's essentially

341
00:23:31,160 --> 00:23:32,160
the same thing.

342
00:23:32,160 --> 00:23:37,320
And this is good because otherwise all our questions and therefore their answers will

343
00:23:37,320 --> 00:23:41,920
look like a robot has written them, which we don't want.

344
00:23:41,920 --> 00:23:47,880
The next pattern, which is surprising comparison, is a little more difficult to templateize.

345
00:23:47,880 --> 00:23:49,680
But here's an example.

346
00:23:49,680 --> 00:23:55,360
Is a metric of a given column greater or less than the same metric of some other column?

347
00:23:55,360 --> 00:24:00,200
So basically I have a metric for a particular column X. Is it greater or less than the same

348
00:24:00,200 --> 00:24:03,200
metric for a particular column Y?

349
00:24:03,200 --> 00:24:06,100
And if there is a significant enough difference, how much is it?

350
00:24:06,100 --> 00:24:08,340
So here are the examples.

351
00:24:08,340 --> 00:24:14,560
What percent of urban population has access to personal protective equipment in the USA

352
00:24:14,560 --> 00:24:16,560
versus in Italy?

353
00:24:16,560 --> 00:24:21,400
You see that these questions have an inherent assumption and then we use them to either

354
00:24:21,400 --> 00:24:24,840
verify or challenge these assumptions.

355
00:24:24,840 --> 00:24:30,620
In this case, the assumption is that urban populations should have easier access to personal

356
00:24:30,620 --> 00:24:32,160
protective equipment.

357
00:24:32,160 --> 00:24:36,800
But by framing this question, we are trying to verify this assumption for two of the worst

358
00:24:36,800 --> 00:24:39,520
COVID hit countries, US and Italy.

359
00:24:39,520 --> 00:24:45,600
If it does turn out that Italy has greater access to per capita or urban population to

360
00:24:45,600 --> 00:24:50,280
PPE than the US, that would be a really interesting insight.

361
00:24:50,280 --> 00:24:52,320
Here is another example.

362
00:24:52,320 --> 00:24:55,160
Are richer countries happier than poorer countries?

363
00:24:55,160 --> 00:24:58,560
Here too the assumption is that richer countries should be happier.

364
00:24:58,560 --> 00:25:01,200
And if we have the data, we can verify this.

365
00:25:01,200 --> 00:25:05,400
By the way, if you actually look at the data, you'll see that this is not really true.

366
00:25:05,400 --> 00:25:09,560
The richest countries in the world are clearly not the happiest countries.

367
00:25:09,560 --> 00:25:12,080
And then here's the third example.

368
00:25:12,080 --> 00:25:17,400
Between demonetization and the COVID-19 lockdown, where did the informal sector of the economy

369
00:25:17,400 --> 00:25:19,960
suffer more losses?

370
00:25:19,960 --> 00:25:25,600
Now both demonetization and the lockdown hurt the informal sector pretty hard.

371
00:25:25,600 --> 00:25:30,680
And the assumption would be, say that the lockdown did more damage to the informal sector

372
00:25:30,680 --> 00:25:32,680
than demonetization.

373
00:25:32,680 --> 00:25:37,880
But there are some studies which show that this is a sampling bias amongst many of us.

374
00:25:37,880 --> 00:25:42,320
The thing is that the lockdown has hurt almost everyone, including you and I.

375
00:25:42,320 --> 00:25:46,960
And because it has hurt me, it's easier for me to assume that this is the worst thing

376
00:25:46,960 --> 00:25:48,240
to happen.

377
00:25:48,240 --> 00:25:53,840
But actually, it is quite likely that demonetization was worse, and I am simply not realizing it

378
00:25:53,840 --> 00:25:58,940
because I myself was not really hurt that much during demonetization.

379
00:25:58,940 --> 00:26:02,300
This time, because of the lockdown, I'm feeling the effect.

380
00:26:02,300 --> 00:26:05,420
That's why I assume that others might be affected too.

381
00:26:05,420 --> 00:26:10,420
So that's an example of how you can challenge something like a sampling bias through things

382
00:26:10,420 --> 00:26:12,740
like this.

383
00:26:12,740 --> 00:26:17,500
The next two patterns, surprising extremes and significant outliers, are very closely

384
00:26:17,500 --> 00:26:18,500
related.

385
00:26:18,500 --> 00:26:22,420
The latter actually is just a special case of the former.

386
00:26:22,420 --> 00:26:25,500
And therefore, their templates too are very similar.

387
00:26:25,500 --> 00:26:30,940
For surprising extremes, we just ask, what is the maximum or minimum of somebody?

388
00:26:30,940 --> 00:26:35,320
In practice, this is as simple as taking one column and sorting the whole data set by

389
00:26:35,320 --> 00:26:36,320
that column.

390
00:26:36,320 --> 00:26:38,580
For example, who is the tallest person in the room?

391
00:26:38,580 --> 00:26:45,660
You could also augment this question by attaching other attributes to that particular question.

392
00:26:45,660 --> 00:26:48,980
For example, what is the gender of the tallest person in the room?

393
00:26:48,980 --> 00:26:51,620
Generally, we expect men to be taller than women.

394
00:26:51,620 --> 00:26:53,740
Is that also true in this particular case?

395
00:26:53,740 --> 00:26:56,660
We can also ask, what is the weight of the tallest person?

396
00:26:56,660 --> 00:26:59,200
Is the tallest person also the heaviest person?

397
00:26:59,200 --> 00:27:05,280
That question makes sense because you would expect weight to increase as height increases.

398
00:27:05,280 --> 00:27:10,120
This pattern, again, allows you to verify or reject a particular belief.

399
00:27:10,120 --> 00:27:15,880
Note that clients can have very strong beliefs about their businesses, and if you can challenge

400
00:27:15,880 --> 00:27:21,040
their beliefs with analytical evidence, they will absolutely love you for it.

401
00:27:21,040 --> 00:27:24,960
Of course, it's also possible to find clients who are not ready to change their beliefs

402
00:27:24,960 --> 00:27:28,820
even in the face of evidence, but that cannot be helped.

403
00:27:28,820 --> 00:27:31,780
It's well known that emotions matter more than facts.

404
00:27:31,780 --> 00:27:34,220
Anyway, back to the point.

405
00:27:34,220 --> 00:27:39,900
The next pattern, which is significant outliers, is just a special case of surprising extremes.

406
00:27:39,900 --> 00:27:43,340
You see that there will always be extremes.

407
00:27:43,340 --> 00:27:48,380
There will always be some minimum or some maximum value for a particular metric, unless

408
00:27:48,380 --> 00:27:52,240
it is a constant, in which case you should not have it in your data at all.

409
00:27:52,240 --> 00:27:58,600
But if the highest or lowest value of every given metric is so ridiculously high or low

410
00:27:58,600 --> 00:28:04,840
that it completely dwarfs the next highest or lowest value, then it's an outlier.

411
00:28:04,840 --> 00:28:10,120
So the template here is like how much greater or less is the highest or lowest value than

412
00:28:10,120 --> 00:28:11,120
the successor.

413
00:28:11,120 --> 00:28:20,280
I know, for example, that say Mumbai has the most flood-related deaths.

414
00:28:20,280 --> 00:28:25,960
That's obvious because Mumbai is notorious for being a low-lying area and not being able

415
00:28:25,960 --> 00:28:29,480
to cope with the monsoons very well.

416
00:28:29,480 --> 00:28:34,120
But if somebody told me that the number of flood-related deaths in Mumbai is five times

417
00:28:34,120 --> 00:28:39,240
as large as something in Kolkata, that would be a significant outlier.

418
00:28:39,240 --> 00:28:43,680
Finally, the last pattern, abnormal distributions.

419
00:28:43,680 --> 00:28:48,720
Here again, we start with some expectation and we frame the question in a way that verifies

420
00:28:48,720 --> 00:28:49,720
that expectation.

421
00:28:49,720 --> 00:28:55,920
For example, what is the expected distribution of a value and does the data match that distribution?

422
00:28:56,880 --> 00:29:01,880
Here is a specific example of this particular kind of question.

423
00:29:01,880 --> 00:29:05,280
In Sholay, is Amitabh Bachchan's coin toss really random?

424
00:29:05,280 --> 00:29:09,640
Now, this question might make sense only to those of you who have seen the movie.

425
00:29:09,640 --> 00:29:13,840
And by the way, if you haven't seen Sholay, this is a good excuse to go and watch it.

426
00:29:13,840 --> 00:29:16,360
Consider it your homework if that helps.

427
00:29:16,360 --> 00:29:21,880
Basically, the point is that if Dharmendra's character in Sholay was a data analyst, he

428
00:29:21,880 --> 00:29:24,560
would have very quickly realized that something is wrong.

429
00:29:24,560 --> 00:29:27,440
The coin toss follows an abnormal distribution.

430
00:29:27,440 --> 00:29:32,760
Anybody would have thought that, how is it 50-50, one person always wins?

431
00:29:32,760 --> 00:29:37,880
Second example, which we have already seen, is that dates of birth across the whole year

432
00:29:37,880 --> 00:29:41,120
are not uniformly distributed, which they should be.

433
00:29:41,120 --> 00:29:47,600
And finally, we hear statements like these so many times, nine out of ten start-ups fail.

434
00:29:47,600 --> 00:29:50,200
We don't know where this figure of nine out of ten comes from.

435
00:29:50,200 --> 00:29:54,120
But if we had data, we could verify it.

436
00:29:54,120 --> 00:30:02,920
So that broadly covers how we can use patterns of insights and how we can use them to frame

437
00:30:02,920 --> 00:30:07,280
questions and perform analysis along those questions.

438
00:30:07,280 --> 00:30:11,080
Before we conclude this section, I want to introduce you to one more framework which

439
00:30:11,080 --> 00:30:15,720
allows us to filter insights even further.

440
00:30:15,720 --> 00:30:18,800
This framework is what we call the bus framework.

441
00:30:18,800 --> 00:30:24,040
It says that insights have to be big, useful, and surprising.

442
00:30:24,040 --> 00:30:26,120
And this is what it means.

443
00:30:26,120 --> 00:30:31,960
Suppose you tell a client that, according to your analysis, if they change a certain

444
00:30:31,960 --> 00:30:35,520
business practice, their revenue will grow by one percent.

445
00:30:35,520 --> 00:30:41,360
Now the client will say that, boss, anyway the fluctuation in my revenue is plus minus

446
00:30:41,360 --> 00:30:42,600
ten percent.

447
00:30:42,600 --> 00:30:44,920
This one percent means nothing at all.

448
00:30:44,920 --> 00:30:48,400
And then on top of that, you are asking me to change the way I run my business, which

449
00:30:48,400 --> 00:30:52,320
is likely going to cost me more than the one percent of my revenue.

450
00:30:52,320 --> 00:30:55,840
So your insight is just not big enough.

451
00:30:55,840 --> 00:30:59,960
To begin with, the analysis has to be statistically very robust.

452
00:30:59,960 --> 00:31:04,200
The probability of your analysis being correct has to be very high.

453
00:31:04,200 --> 00:31:09,440
But if the impact of that analysis is low, then it's just not big enough.

454
00:31:09,440 --> 00:31:13,760
Let's say that we make the following statement to our client.

455
00:31:13,760 --> 00:31:19,560
We are 99 percent sure that if you do X, your revenue will increase by 10 percent.

456
00:31:19,560 --> 00:31:24,080
But we are only 70 percent sure that if you do Y, then your revenue will increase by

457
00:31:24,080 --> 00:31:25,080
30 percent.

458
00:31:25,080 --> 00:31:29,520
In this scenario, the client is likely to adopt the second recommendation because the

459
00:31:29,520 --> 00:31:31,480
first is just not big enough.

460
00:31:31,480 --> 00:31:36,400
And even if you're only 70 percent confident about the second scenario, that still might

461
00:31:36,400 --> 00:31:39,080
be a risk worth taking.

462
00:31:39,080 --> 00:31:45,880
Now the second question to ask is, is the insight useful or is it actionable?

463
00:31:45,880 --> 00:31:51,440
Suppose a venture capitalist were to hire you and they ask you to study startups and

464
00:31:51,440 --> 00:31:57,440
small businesses and their likelihood to succeed so that you can give some advice on where

465
00:31:57,440 --> 00:31:59,040
they should invest their money.

466
00:31:59,040 --> 00:32:04,760
Now you do your analysis and you discover the biggest driver of a startup success is

467
00:32:04,760 --> 00:32:06,560
the country of that startup.

468
00:32:06,560 --> 00:32:11,840
The highest impact on the success of a startup is that of the country because presumably

469
00:32:12,080 --> 00:32:16,560
Ease of doing business matters more than any other factor and ease of doing business is

470
00:32:16,560 --> 00:32:22,680
directly related to the economy, to the laws and a lot of establishment related things

471
00:32:22,680 --> 00:32:24,880
in general.

472
00:32:24,880 --> 00:32:29,080
So then you go to your client and say that they should invest in startups which are in

473
00:32:29,080 --> 00:32:35,240
New Zealand or Singapore because those countries have the highest ease of business rankings.

474
00:32:35,240 --> 00:32:40,120
Your client might say that, look, these markets are already captured by other investors.

475
00:32:40,120 --> 00:32:41,820
That is not my market at all.

476
00:32:41,820 --> 00:32:44,020
What can you tell me about my market?

477
00:32:44,020 --> 00:32:49,100
So this is an example of something which is not useful, something which is not actionable.

478
00:32:49,100 --> 00:32:54,300
Another example of this is one analysis that we were doing on cricket scores.

479
00:32:54,300 --> 00:33:01,100
So apparently if you try to quantify the performance of a particular batsman or a particular bowler,

480
00:33:01,100 --> 00:33:05,780
it seems like the country or the team that they play for has the highest impact in how

481
00:33:05,780 --> 00:33:06,780
well they perform.

482
00:33:06,780 --> 00:33:10,980
There are certain countries whose players overall will perform better, India, Pakistan,

483
00:33:10,980 --> 00:33:12,700
South Africa and so on.

484
00:33:12,700 --> 00:33:13,700
Not surprising.

485
00:33:13,700 --> 00:33:19,220
But then, you know, if you were actually to turn this into a data analysis problem and

486
00:33:19,220 --> 00:33:24,940
say that, okay, what do I need to do in order to improve my performance?

487
00:33:24,940 --> 00:33:29,940
You might easily be able to say that, hey, according to my analysis, since country is

488
00:33:29,940 --> 00:33:35,620
the biggest driver in performance of cricket scores, why don't you go and play for a different

489
00:33:35,620 --> 00:33:36,620
country?

490
00:33:36,620 --> 00:33:39,180
Now that's again an example of an insight that is not actionable.

491
00:33:39,180 --> 00:33:46,100
I can't just walk up to some other, a better team and ask them to let me play for them.

492
00:33:46,100 --> 00:33:50,220
Finally, don't tell your audience something that they already know.

493
00:33:50,220 --> 00:33:52,740
Tell them something that is not obvious.

494
00:33:52,740 --> 00:33:58,540
So if an insight meets all these three criteria, then you can use it.

495
00:33:58,540 --> 00:34:05,940
The problem with the big, useful, surprising framework though, is that it is very subjective.

496
00:34:05,940 --> 00:34:09,860
After all, how will you know if the client knows something or not?

497
00:34:09,860 --> 00:34:15,140
How will you know if something that seems big to you is also big for the client?

498
00:34:15,140 --> 00:34:20,060
Unfortunately, this kind of acumen comes only with experience and practice.

499
00:34:20,060 --> 00:34:22,980
But that doesn't mean that we cannot use this framework.

500
00:34:22,980 --> 00:34:24,660
We can always talk to our clients.

501
00:34:24,660 --> 00:34:30,060
We can interview any other stakeholders, interview them about what sort of things count as being

502
00:34:30,060 --> 00:34:31,060
useful or surprising.

503
00:34:31,060 --> 00:34:34,740
We can always do multiple rounds before we make the final presentation.

504
00:34:34,740 --> 00:34:40,940
And that's actually a good thing because that shows engagement also to some extent.

505
00:34:40,940 --> 00:34:46,780
In any case, we can always exploit the inherent subjectivity of the problem.

506
00:34:46,780 --> 00:34:51,500
The fact that these are subjective measures can actually turn into an advantage for us.

507
00:34:51,500 --> 00:34:56,260
For example, we can define a simple scale of low, medium and high.

508
00:34:56,260 --> 00:35:03,300
And for each insight, we rank it as low, medium and high across all of the bus metrics.

509
00:35:03,300 --> 00:35:09,100
And then we can create our own filtering rule, like if any insight is medium or high

510
00:35:09,100 --> 00:35:13,940
on all three bus factors, then we will include it in the final report.

511
00:35:13,940 --> 00:35:15,700
So here are a few examples.

512
00:35:15,700 --> 00:35:22,060
Let's say we figure out that project managers get paid five times as much as data analysts.

513
00:35:22,060 --> 00:35:23,060
Is this big?

514
00:35:23,060 --> 00:35:24,060
Absolutely.

515
00:35:24,060 --> 00:35:27,260
Five times is a big magnification.

516
00:35:27,260 --> 00:35:28,260
Is it useful?

517
00:35:28,260 --> 00:35:29,260
Well, that depends.

518
00:35:29,260 --> 00:35:30,260
Useful to whom?

519
00:35:31,220 --> 00:35:36,980
It is somewhat useful to data analysts because they can use this insight to try and get either

520
00:35:36,980 --> 00:35:41,300
promoted to a project manager or go and get an MBA.

521
00:35:41,300 --> 00:35:43,660
Is it useful to employers or recruiters?

522
00:35:43,660 --> 00:35:44,740
Definitely.

523
00:35:44,740 --> 00:35:48,300
Because if this is the market standard, then they know roughly how much they should pay

524
00:35:48,300 --> 00:35:52,140
for which particular designation or which particular role.

525
00:35:52,140 --> 00:35:54,860
Finally, is it surprising?

526
00:35:54,860 --> 00:35:55,860
Not really.

527
00:35:55,860 --> 00:36:02,900
People may not know the 5x multiplication factor very precisely, but they know that

528
00:36:02,900 --> 00:36:04,180
this is generally true.

529
00:36:04,180 --> 00:36:08,660
That is, project managers do get paid much more than data analysts do.

530
00:36:08,660 --> 00:36:13,540
They may not know how much, but they know that that order relationship exists.

531
00:36:13,540 --> 00:36:19,620
The second example here is that business analysts get paid twice as much as data analysts.

532
00:36:19,620 --> 00:36:23,820
Now, this has a high rating across all columns.

533
00:36:23,820 --> 00:36:25,780
It's definitely a big figure.

534
00:36:25,780 --> 00:36:32,260
It's useful because it will prompt data analysts to label themselves as business analysts.

535
00:36:32,260 --> 00:36:37,460
And it is surprising too since you would assume that business analysts and data analysts do

536
00:36:37,460 --> 00:36:39,580
almost the same kind of work.

537
00:36:39,580 --> 00:36:45,060
So broadly, the idea is that you can rank each insight by how it performs within the

538
00:36:45,060 --> 00:36:52,220
bus framework and use this as a checklist for what to include in the final analysis.

539
00:36:52,220 --> 00:36:56,180
Feel free to pause the video here and take a look through the rest of the examples as

540
00:36:56,180 --> 00:37:01,020
well.

541
00:37:01,020 --> 00:37:06,660
To summarize this module, we have talked in detail about how analysis alone doesn't generate

542
00:37:06,660 --> 00:37:07,660
insights.

543
00:37:07,660 --> 00:37:12,020
In fact, analysis is a superset of insights.

544
00:37:12,020 --> 00:37:14,460
Analysis has to be filtered into insights.

545
00:37:14,460 --> 00:37:17,380
And in order to do that, we covered two methods.

546
00:37:17,380 --> 00:37:22,980
The first one we call patterns of insights and we talked about the five patterns of insights

547
00:37:22,980 --> 00:37:27,620
and how they can be used to frame questions, which in turn can drive our analysis in the

548
00:37:27,620 --> 00:37:28,620
first place.

549
00:37:28,620 --> 00:37:36,060
Finally, we learned how we can rank each insight as a big useful or surprising and how to use

550
00:37:36,060 --> 00:37:42,080
these rankings as a second set of filters against our insights.

551
00:37:42,080 --> 00:37:46,260
In our forthcoming interactive sessions, we are going to go through many examples of how

552
00:37:46,340 --> 00:37:50,860
we can actually apply all of these things in practice to the point that they become

553
00:37:50,860 --> 00:37:52,460
habits.

554
00:37:52,460 --> 00:37:54,820
So till then, all the best and see you soon.

