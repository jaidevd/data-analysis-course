1
00:00:00,000 --> 00:00:06,200
Welcome back. So now that we have done some little bit of manipulation and cleaning on

2
00:00:06,200 --> 00:00:11,720
the original dataset, let's pick up where we left off and try to see if we can do any

3
00:00:11,720 --> 00:00:17,360
serious data exploration, any statistical analysis on the dataset. So as usual, let's

4
00:00:17,360 --> 00:00:23,520
import the Pandas library and the plotting library, which is matplotlib, and run the

5
00:00:23,520 --> 00:00:29,320
dataset to import them. Now, as usual, at the end of the last notebook, you must have

6
00:00:29,320 --> 00:00:34,400
seen that we had written this data to this particular path. We had a folder called data

7
00:00:34,400 --> 00:00:39,840
within which we created this file caggle clean, and we're just going to read it back again

8
00:00:39,840 --> 00:00:46,120
and look at the first five rows of the dataset. So you see, we still have the same problem.

9
00:00:46,120 --> 00:00:51,760
That is, our first column is still text and our header is obviously already text. So this

10
00:00:52,320 --> 00:00:56,000
data is still not completely numerical. If we were to again take the histogram of all the

11
00:00:56,880 --> 00:01:01,680
durations in this dataset, we'd still run into the same problem where it'll say that it can't

12
00:01:01,680 --> 00:01:06,880
compare these values against a particular string. And that is why we essentially have to figure out

13
00:01:06,880 --> 00:01:12,880
a way to drop this entire row and see where we can include that data so that only we are

14
00:01:14,160 --> 00:01:21,040
restricted to crunching the data that is present in rows one onwards. So let's see what we can do

15
00:01:21,040 --> 00:01:26,720
that. Now, one of the arguments that the read CSV function accepts is something called a header.

16
00:01:26,720 --> 00:01:30,960
Now, by default, if you don't provide that argument, when we read the data for the first

17
00:01:30,960 --> 00:01:36,640
time using the read CSV function, we did not tell it what its header was supposed to be.

18
00:01:36,640 --> 00:01:41,360
So usually it simply takes the first row as the header. It assumes that there is a header

19
00:01:41,360 --> 00:01:45,680
and the first row is the header. If there is no header, then you explicitly have to tell it that

20
00:01:45,680 --> 00:01:49,760
there is no header. But of course, in this particular case, our header is not just one

21
00:01:49,760 --> 00:01:55,600
column. It's actually two columns. You see that this row is actually a header, but it has started

22
00:01:55,600 --> 00:02:00,320
coming in as the first column. So what we're doing here is we're telling it that take the

23
00:02:00,320 --> 00:02:05,120
zeroth row as the header and also take the first row as the header. So basically our header should

24
00:02:05,120 --> 00:02:12,880
span two rows. So let's do that and see what it shows us again. There we have, you see that now

25
00:02:12,880 --> 00:02:18,000
our header is spanning this row and the first row as well. And now the data starts correctly coming

26
00:02:18,000 --> 00:02:24,080
in from the zeroth row onwards. But at the same time, you see that our header has grown a little

27
00:02:24,080 --> 00:02:30,480
complicated. For example, earlier when I wanted to refer to this particular column, I just used

28
00:02:30,480 --> 00:02:35,920
call name as the string time from start to finish in seconds. And I use that call name to access the

29
00:02:35,920 --> 00:02:40,000
data. As you know, I can just put the name of a column within the square brackets and call it on

30
00:02:40,000 --> 00:02:46,720
the data frame to get a particular column out of the data frame. But here, since my data frame has

31
00:02:46,720 --> 00:02:52,480
gotten a little complicated because of the header, I don't know what to do with this extra text. So

32
00:02:52,480 --> 00:02:56,640
here's one way to do it. If we simply look at the columns, here's what it's going to give me.

33
00:02:56,640 --> 00:03:02,080
It basically tells me that it's a multi-index instead of a single index as we have seen earlier.

34
00:03:02,080 --> 00:03:07,120
What a multi-index means is that the data is being indexed by two values, not one value. So

35
00:03:07,120 --> 00:03:13,200
essentially earlier we had rows or rather columns which were indexed just by this particular header.

36
00:03:13,200 --> 00:03:18,720
Now we have columns which are being indexed by two headers at a time, not one header. So that's

37
00:03:18,720 --> 00:03:23,760
what a multi-index means. It basically says that time from start to finish is also a descriptor,

38
00:03:23,760 --> 00:03:28,720
as well as duration in seconds is also another descriptor. So if you look at the first column,

39
00:03:29,440 --> 00:03:34,800
which is basically q1, this was the zeroth column. If you look at the first column which is q1,

40
00:03:35,600 --> 00:03:40,720
it will basically give me this data set. Now you see that this is not a single column. It's

41
00:03:40,720 --> 00:03:45,920
basically creating another column which has a column within itself and that is basically

42
00:03:45,920 --> 00:03:51,520
the second header. So it's sort of a hierarchy. You see that q1 is basically the top level column

43
00:03:51,520 --> 00:03:56,080
and under that there is another column. So that's how it's interpreting that. And if we were actually

44
00:03:56,080 --> 00:04:02,000
to pick those two columns together, then it would give me the raw column itself completely.

45
00:04:02,000 --> 00:04:06,640
And if you look at the name of this column, you see that it's giving us a tuple. From the first

46
00:04:06,640 --> 00:04:11,360
video that we saw on python, we know that tuple is basically one of the data structures. And it

47
00:04:11,360 --> 00:04:16,080
says that this tuple has two elements, one is q1 and the second is what is your age in years.

48
00:04:16,080 --> 00:04:21,680
And that actually matches this. We see that q1 and what is your age in years are basically two

49
00:04:21,680 --> 00:04:27,920
different column headers and both of them here are required to refer to the same data. This is

50
00:04:28,640 --> 00:04:33,600
very complicated. So what we are going to do is try and make it more manageable. We are going to

51
00:04:33,600 --> 00:04:37,600
keep only the first level of indexing on the data frame and the second we are going to keep

52
00:04:37,600 --> 00:04:42,560
only for reference. So basically what we are going to do is we are going to tell pandas that only

53
00:04:42,560 --> 00:04:48,960
consider this particular row as the header. And this we are going to basically delete, but we are

54
00:04:48,960 --> 00:04:54,720
going to keep it separately somewhere so that later if we forget what q6 is, we can always look it up

55
00:04:54,720 --> 00:05:00,800
in a particular lookup table or dictionary where we can say that okay I might have forgotten what

56
00:05:00,800 --> 00:05:05,680
q6 is, but this is what it means. q6 corresponds to the question which asks what is the size of

57
00:05:05,680 --> 00:05:11,520
the company where you are employed. So in order to keep accessing the data, we are simply going to

58
00:05:11,520 --> 00:05:16,480
keep these codes, but their meanings we are going to throw away from the table but maintain in a

59
00:05:16,480 --> 00:05:22,560
separate dictionary somewhere. So let's try and do that. So basically one quick way to do that is

60
00:05:22,560 --> 00:05:29,360
use something called a dictionary comprehension. As we know, we can create dictionaries by using

61
00:05:29,360 --> 00:05:38,640
something like this. So as you know we can create dictionaries using a pair of braces and I can say

62
00:05:38,640 --> 00:05:48,400
something like this is my key and my value and then I have another key and another value something

63
00:05:48,400 --> 00:05:53,120
like that. So that basically creates my dictionary. One quick way to create a dictionary is using

64
00:05:53,120 --> 00:05:58,400
something called a dictionary comprehension. So what it's going to do is essentially you keep the

65
00:05:58,400 --> 00:06:03,440
braces as they are but inside instead of writing a set of key value pairs. Now of course we can't

66
00:06:03,440 --> 00:06:09,680
write the whole set of 246 key value pairs as I present here. What we want essentially is a dictionary

67
00:06:09,680 --> 00:06:14,880
where the question codes are my keys and the text of the question is the value. So I can just put

68
00:06:14,880 --> 00:06:19,280
the question code in that dictionary and get the question corresponding to that code. That's the

69
00:06:19,280 --> 00:06:25,040
kind of dictionary I want. One quick way to do that is to use a dictionary comprehension where I say

70
00:06:25,040 --> 00:06:31,920
that k colon v for k comma v in df.columns. Now df.columns since it is a multi-index is going to

71
00:06:31,920 --> 00:06:36,000
iterate over both these lists. The first level which is the question code and the second level

72
00:06:36,000 --> 00:06:41,600
which is the question text. So k becomes my question code and v becomes my question text and that

73
00:06:41,600 --> 00:06:47,280
gives me this whole dictionary. It basically is a massive python dictionary which says that q1

74
00:06:47,280 --> 00:06:52,640
is a key which corresponds to what is your age, q2 is a key which corresponds to what is your gender,

75
00:06:52,640 --> 00:06:57,120
q4 is the question which corresponds to what is the highest level of formal education that you have

76
00:06:57,120 --> 00:07:01,760
and so on. So let's scroll all the way down it's going to be pretty large because

77
00:07:02,320 --> 00:07:09,360
there are 246 questions. Much easier way is to simply use the dict function. Remember that just

78
00:07:09,360 --> 00:07:14,000
like we use the list function to create lists you can use the dict function to create dictionaries

79
00:07:14,000 --> 00:07:19,920
and you can simply call this on df.columns. df.columns is simply a multi-index column which

80
00:07:19,920 --> 00:07:25,280
means that it has key values everywhere and we do that we basically get the same data again.

81
00:07:26,160 --> 00:07:31,120
So this is an easier way to do it rather than using dictionary complications. Of course one

82
00:07:31,120 --> 00:07:38,000
of them might be suitable for a certain application and you should be aware of what both of them do.

83
00:07:38,000 --> 00:07:42,480
So now since we know which codes corresponds to which question we can get rid of the second

84
00:07:42,480 --> 00:07:47,280
level of columns. So what we're going to do is create a new dictionary called question codes

85
00:07:47,360 --> 00:07:52,000
and that is essentially this particular dictionary. You have the keys as the question

86
00:07:52,000 --> 00:07:57,280
codes and the values as the text of the question and that creates my question codes and after that

87
00:07:57,280 --> 00:08:02,640
I'm going to create a new data frame which I'll be calling xdf simply as a modification of my

88
00:08:02,640 --> 00:08:09,600
original data frame and I'm going to say that drop level. So drop level is a function which tells

89
00:08:10,400 --> 00:08:17,360
pandas to drop a level of index from the data set. If we go all the way back to our data frame

90
00:08:18,000 --> 00:08:23,120
there's actually a lot of text here what we can do is simply remove this. So I can say clear output

91
00:08:23,120 --> 00:08:27,120
and here too I can say clear output. So we have a little more room to scroll around now.

92
00:08:28,000 --> 00:08:33,200
If we go back to the data we see that my header is present in two levels. This is the first level

93
00:08:33,200 --> 00:08:37,920
and this is the second level. Now obviously that means that this is the zeroth level and this is

94
00:08:37,920 --> 00:08:44,640
the first level. So what I'm going to simply do is ask pandas to drop the first level along the

95
00:08:44,640 --> 00:08:50,880
first axis. Remember we have seen that when we talk of the zeroth axis we are talking about

96
00:08:50,880 --> 00:08:55,760
row wise stuff whereas when we talk about the first axis we are talking about column wise stuff.

97
00:08:55,760 --> 00:09:01,360
Here our second level of indexing is happening along the columns not along the rows. Remember

98
00:09:01,360 --> 00:09:06,960
that the rows are still indexed with a single number. So we are simply going to drop that first

99
00:09:06,960 --> 00:09:14,000
thing and there it is. Now when we look at xdf.head we have a lot cleaner data set. We just have the

100
00:09:14,000 --> 00:09:18,960
question codes and we have the values. But of course now what if I how do I find out what q1

101
00:09:18,960 --> 00:09:23,520
means? Well I've already created a dictionary called question codes right and there I can just

102
00:09:24,160 --> 00:09:30,240
put in q1 and I say that okay q1 does correspond to what is your age and then q2 also correspondingly

103
00:09:30,960 --> 00:09:38,160
corresponds to this question what is your gender. So there it is. We have a column q2 which

104
00:09:38,160 --> 00:09:44,080
corresponds to gender and here's an exercise. How would we find the gender ratio of this survey?

105
00:09:45,120 --> 00:09:50,160
So pause the video for a while take a minute to think about it and do the exercise. We already

106
00:09:50,160 --> 00:09:55,360
know how to get a particular column out of pandas. We just do df square brackets put in the name of

107
00:09:55,360 --> 00:10:00,080
the column that gives you the whole column. So one of the things we might want to do simply is to

108
00:10:00,080 --> 00:10:05,520
calculate all the males and females and find their ratio essentially. So essentially the point is

109
00:10:05,520 --> 00:10:09,760
counting how many males are there counting how many females are there. There could be other

110
00:10:09,760 --> 00:10:16,960
genders also so be aware of that and see what are the proportions of different genders. So feel free

111
00:10:16,960 --> 00:10:21,440
to look around feel free to google and here we are going to introduce some of the more

112
00:10:22,160 --> 00:10:26,000
advanced pandas functionality and we'll do that as soon as we resume.

113
00:10:26,000 --> 00:10:30,640
So let's take a shot at how we would do this. In order to find the gender ratio that is in what

114
00:10:30,640 --> 00:10:35,360
proportion the different genders are present. First of all I have to find out what all different

115
00:10:35,360 --> 00:10:41,520
values are there. So male and female I can already see here but note that I am looking at all I am

116
00:10:41,520 --> 00:10:48,960
looking at only five rows out of the 19,718 rows. So let's say that we want to find out all the

117
00:10:49,600 --> 00:10:54,320
possible values that people have put in under Q2 which is gender. So let's say that we create an

118
00:10:54,320 --> 00:11:00,400
empty list called gender and then we are going to iterate through this row. So what I can do then is

119
00:11:00,400 --> 00:11:10,480
for gender in df Q2 which is genders. By the way dot values is what actually lets you iterate through

120
00:11:10,480 --> 00:11:17,600
the values that's essentially the list of genders. So let's say that we want to find out all the

121
00:11:17,600 --> 00:11:22,480
values that's essentially the list which is the underlying data structure of a particular column

122
00:11:22,480 --> 00:11:26,480
and I'm going to say that if gender is already in genders

123
00:11:29,360 --> 00:11:36,080
remember that in is something we use to check membership in a list. So this is a list which

124
00:11:36,080 --> 00:11:40,160
is currently empty and we are going to keep on accumulating the different genders that we see.

125
00:11:40,160 --> 00:11:46,640
So if the gender that we have in a particular row of Q2 is already in this list then we do nothing

126
00:11:46,640 --> 00:11:55,920
which means continue the iteration or then if it is not inside the gender then we can simply do

127
00:11:55,920 --> 00:12:02,400
genders not append of gender. So we are going to keep on appending to this dictionary and at the

128
00:12:02,400 --> 00:12:10,320
end we are just going to say print genders. So again to take a look at this in a little more

129
00:12:10,320 --> 00:12:14,800
detail we have created an empty list which contains all the genders in that data set

130
00:12:14,800 --> 00:12:18,800
and we are going to iterate through that column and we say that if the gender that is in a

131
00:12:18,800 --> 00:12:24,160
particular iteration is already present in my list if gender in genders then I simply continue

132
00:12:24,160 --> 00:12:29,040
I don't do anything but if it is not there then I'm simply going to append that gender to my

133
00:12:29,040 --> 00:12:34,160
genders and then at the end once the for loop is done I'm going to print out the list of all the

134
00:12:34,160 --> 00:12:41,120
accumulated genders. So there it is. So we see that we have four things we have something like

135
00:12:41,120 --> 00:12:45,680
there is male there is female there is one option says prefer to self-describe

136
00:12:45,680 --> 00:12:52,480
and the other one says prefer not to say. Now one easy way of doing that is instead of all of this

137
00:12:52,480 --> 00:12:59,760
for loop jugglery I can simply do df Q2 which is my column and I can simply do a unique statement on

138
00:12:59,760 --> 00:13:09,440
it. Well same data for attribute oh of course we had xdf not simply df because xdf was something

139
00:13:09,440 --> 00:13:15,760
that we created after dropping the first level of hierarchy so let's do it on xdf and there it is.

140
00:13:15,760 --> 00:13:21,600
So we get the same four values back again this time without using the for loop. So this is a method

141
00:13:21,600 --> 00:13:26,800
that is already built into pandas use it whenever you can whenever you find yourself writing too

142
00:13:26,800 --> 00:13:30,960
much extra code that's probably a good signal that you might just want to look up in the

143
00:13:30,960 --> 00:13:36,560
documentation and try to find something that does the job for you ready mate. So there it is these

144
00:13:36,560 --> 00:13:42,080
are all the unique genders. Now the next thing that we have to do is basically count how many

145
00:13:42,080 --> 00:13:46,640
males are there how many females are there how many people prefer to self-describe and how many

146
00:13:46,640 --> 00:13:53,040
people prefer not to say. So what we are going to do is simply create a list of counts and let us

147
00:13:53,040 --> 00:14:01,680
say that I create a dictionary called counts where I say male is zero female is zero

148
00:14:05,600 --> 00:14:13,520
prefer to self-describe is a key again which is zero and prefer not to say

149
00:14:13,520 --> 00:14:21,440
is yet another key which has a value of zero. Now what I'm going to do is again iterate through

150
00:14:21,440 --> 00:14:32,240
this list so let's say for gender in xdf of q2.values I'm just going to say my count is basically

151
00:14:34,320 --> 00:14:38,800
from the counts I'm just going to get the gender. Now I know that this value of gender is going to

152
00:14:38,800 --> 00:14:43,120
be one of the keys present here can't be a fifth key because I already know what are all the unique

153
00:14:43,120 --> 00:14:47,440
values that I have and those are the ones that I've put in this dictionary. Now since counts is

154
00:14:47,440 --> 00:14:52,400
my dictionary and gender is one of the values in the row whenever I get this initially it's going

155
00:14:52,400 --> 00:14:58,080
to be zero and since I have found that particular gender I'm just going to increment my count so

156
00:14:58,080 --> 00:15:05,120
I'm just going to take whatever value is already accumulated there and add one to it and then I'm

157
00:15:05,120 --> 00:15:14,400
going to put that value back into the dictionary. So essentially this is getting the count access

158
00:15:14,400 --> 00:15:20,080
this step increments the count and this step steps back the count adds the count back to the

159
00:15:20,080 --> 00:15:30,320
dictionary and then I say print counts. So there we are we have 15,890 males 3,178 males

160
00:15:30,320 --> 00:15:38,240
48 people prefer to self-describe and 303 people prefer not to say. So that's basically the total

161
00:15:39,200 --> 00:15:46,240
number of all types of genders that we have. One quick way to simply get the ratio is divide each

162
00:15:46,240 --> 00:15:52,000
of these values by the total number of rows that are present and like we know the total number of

163
00:15:52,000 --> 00:16:00,640
rows that are present can be obtained by saying xdf.shape where 19419 is basically the number of

164
00:16:00,640 --> 00:16:07,440
filtered rows and 246 is still the number of columns. So I have something like in rows note

165
00:16:07,440 --> 00:16:12,240
that this is a tuple and the first value in the tuple is going to be the number of rows so I can

166
00:16:12,240 --> 00:16:18,640
say n rows equal to xdf.shape of zero because that gets me the zeroth element of the tuple

167
00:16:18,640 --> 00:16:23,840
and then I'm going to say that for gender and count

168
00:16:27,680 --> 00:16:32,560
that's probably a bad idea we don't want to iterate over a dictionary and then do it again.

169
00:16:32,560 --> 00:16:35,120
So at the end of it what we could simply do is

170
00:16:36,720 --> 00:16:46,960
for gender count in counts.items print gender and then what we're going to do is divide count

171
00:16:47,840 --> 00:16:55,360
by the number of rows multiplied by 100 and say that that's my percentage basically this is the

172
00:16:55,360 --> 00:17:01,520
percentage for a given gender. So let's run that and now we know that 81 percent are males 16 percent

173
00:17:01,520 --> 00:17:10,320
are females 0.24 percent prefer to self-describe and 1.5 percent prefer not to say. Now again here

174
00:17:10,320 --> 00:17:15,840
as far as possible we would like to avoid all of this jugglery again so one quick way to avoid

175
00:17:15,840 --> 00:17:22,560
this is basically just to do something like this I simply take xdf I select the column that I'm

176
00:17:22,560 --> 00:17:28,240
interested in and I use a new method called value underscore counts what it's going to do is it's

177
00:17:28,240 --> 00:17:32,880
simply going to count each value so there it is and we have the same things again and again

178
00:17:32,880 --> 00:17:42,640
we have the male which occurs 15,890 times females occur 3,178 times prefer not to say comes 303

179
00:17:42,640 --> 00:17:48,640
times and those who prefer to self-describe are 48 in number. Now one quick way to convert this

180
00:17:48,640 --> 00:17:56,720
into a percentage is to divide that by n rows which we have already calculated as xdf dot shape of 0

181
00:17:56,720 --> 00:18:02,880
and multiplied by 100 and that gives me the same percentages that I earlier calculated so what we

182
00:18:02,880 --> 00:18:09,760
see here is an example of how you can just use one line of pandas code to you know counter something

183
00:18:09,760 --> 00:18:15,920
like eight lines of pure python code. Now another interesting thing is that you can since these are

184
00:18:15,920 --> 00:18:20,480
you know percentages they all add up to 100 percent we can use something like a pie chart to

185
00:18:20,480 --> 00:18:26,000
plot this so in order to do that remember that this is a panda series and panda series and data

186
00:18:26,000 --> 00:18:31,200
frames both support excellent data visualization and plotting capabilities so I can just do

187
00:18:31,200 --> 00:18:40,720
something like this plot let's say kind is pi and there we are so we have the male we have the females

188
00:18:40,720 --> 00:18:45,760
and the miniscule proportions of people who say that they prefer to self-describe or prefer not

189
00:18:45,760 --> 00:18:52,880
to say so this is one quick way to see what sort of proportions you have in your data you basically

190
00:18:52,880 --> 00:18:57,840
use value counts you can convert it into a percentage and then plot it as a pie chart

191
00:18:57,840 --> 00:19:01,920
you can of course plot it as a bunch of other things also we can also plot it as a bar chart

192
00:19:01,920 --> 00:19:06,640
but ultimately the bar chart doesn't exactly apply here because we are looking at proportions

193
00:19:06,640 --> 00:19:11,680
so part of whole so that's why the pie chart might make sense a lot of data visualization experts

194
00:19:12,400 --> 00:19:18,880
hate pie charts for good reason there are always good or bad places to use certain kinds of charts

195
00:19:18,880 --> 00:19:23,440
so be careful with what visualization you pick for what kind of a problem in this case it works

196
00:19:24,640 --> 00:19:31,600
so there it is one easy way to look at different unique values and to summarize them is by looking

197
00:19:31,600 --> 00:19:37,200
at different how many times each value occurs and convert it into percentages however this only works

198
00:19:37,200 --> 00:19:41,520
if the number of unique values is very small remember that we had only four unique values here

199
00:19:41,520 --> 00:19:47,920
in about 19 000 rows if we had something like 5000 unique values in 19 000 rows then this pie

200
00:19:47,920 --> 00:19:53,920
chart would have looked like it has too many spokes so we don't want that either so make sure

201
00:19:53,920 --> 00:19:58,960
that before you start doing an analysis of this kind the number of unique values that you have

202
00:19:58,960 --> 00:20:04,720
is fairly small one quick way to simply check the number of unique values not the unique values

203
00:20:04,720 --> 00:20:12,960
themselves is to take the column which we do like this and call n unique on it earlier we called

204
00:20:13,040 --> 00:20:18,080
unique to get the unique values themselves we can call n unique to get the number of unique

205
00:20:18,080 --> 00:20:23,280
values and we know that though there are four unique values over here male female prefer to

206
00:20:23,280 --> 00:20:28,720
self-describe and prefer not to say right so there we are one quick way to get a bunch of

207
00:20:28,720 --> 00:20:33,440
different summary statistics or univariate statistics as we have studied before for a given

208
00:20:33,440 --> 00:20:40,080
column is to use the describe function that is provided for every column in a pandas data frame

209
00:20:40,080 --> 00:20:46,160
so when i say i just want to do q2 dot describe here it is it straight away tells me that the

210
00:20:46,160 --> 00:20:50,640
count is nineteen thousand four hundred and nineteen there are four unique values the most

211
00:20:50,640 --> 00:20:56,640
frequent is male which has a frequency of this much so we know that males awkward 81 percent of

212
00:20:56,640 --> 00:21:02,240
the times and that corresponds to out of nineteen thousand four hundred and nineteen fifteen thousand

213
00:21:02,240 --> 00:21:06,960
eight hundred and ninety of course remember that q2 was a categorical column we have already covered

214
00:21:06,960 --> 00:21:12,480
what are numerical ordinal and categorical columns q2 was an uh categorical column therefore

215
00:21:12,480 --> 00:21:17,120
its descriptive statistics are going to be different than say the descriptive statistics

216
00:21:17,120 --> 00:21:22,000
of a numerical column now the only numerical column we have in our data set is time from

217
00:21:22,000 --> 00:21:27,520
start to finish so let's try to see what describe would tell us if i try to do a describe on that

218
00:21:28,320 --> 00:21:34,800
so let's say i do an xdf and then i put in time from start to finish and then do a describe

219
00:21:36,960 --> 00:21:41,280
here you see it gives me a very different set of statistics because it's a numerical column of course

220
00:21:41,280 --> 00:21:46,560
count is still valid over here but now it has a mean a standard deviation it has a minimum value

221
00:21:46,560 --> 00:21:50,240
of course you see that the minimum value is 60 because we had already cleaned the data in the

222
00:21:50,240 --> 00:21:56,000
previous video then 25 percent 50 percent and 75 percent these are essentially the percentile

223
00:21:56,000 --> 00:22:02,880
what this means is that 25 percent of all values are below 350 50 percent of them are below 547

224
00:22:02,880 --> 00:22:11,200
and 75 percent of them are below 942 and the maximum value is 843 612 seconds

225
00:22:12,240 --> 00:22:18,560
no idea what this guy was doing but the bigger point that we're trying to make here is that

226
00:22:18,560 --> 00:22:23,280
the output of describe is different for different types of columns for numerical columns it's going

227
00:22:23,280 --> 00:22:29,280
to give me summary statistics for categorical column also it's going to give me some summary

228
00:22:29,280 --> 00:22:32,320
statistics but those statistics themselves are going to be different

229
00:22:35,200 --> 00:22:41,600
so these were some examples of univariate data analysis let's take a look at how we can perform

230
00:22:41,600 --> 00:22:48,480
a little bit of bivariate data analysis as well now specifically we have created a data set which

231
00:22:48,480 --> 00:22:54,880
is clean and we have the header columns properly formatted let's take a look at the indian subset

232
00:22:54,880 --> 00:22:59,920
of the data as you remember we have a dictionary called question codes which maps the question

233
00:22:59,920 --> 00:23:04,800
code to the actual question so let's take a look at what q3 is and we say that you know

234
00:23:05,520 --> 00:23:11,920
question q3 corresponds to country of residence so what we're going to do is take that subset

235
00:23:11,920 --> 00:23:17,360
of the data which corresponds to all the indian respondents if you remember from the previous

236
00:23:17,360 --> 00:23:22,880
exercise we had performed filtering on the data set which contained marks and attempts of multiple

237
00:23:22,880 --> 00:23:28,000
students for a person called emily so here we are going to apply the same syntax we are going to

238
00:23:28,000 --> 00:23:34,160
select the column q3 which is country of residence we are going to compare it with the string india

239
00:23:34,160 --> 00:23:39,360
and that's going to return a bunch of true and false values and those true and false values are

240
00:23:39,360 --> 00:23:46,880
what we are going to use to pick the rows which correspond to indian respondents out of the xdf

241
00:23:46,880 --> 00:23:53,680
data frame and that subset of the data we are going to create into another variable which we

242
00:23:53,680 --> 00:24:00,080
will just call india and then let's simply do india.head and there we are we see that the first

243
00:24:00,080 --> 00:24:04,640
respondent is from india the fourth is from india the sixth is from india the tenth is from india

244
00:24:04,640 --> 00:24:11,280
and so on so we again are looking at just five rows because we have done ahead and 246 columns

245
00:24:11,280 --> 00:24:17,200
so these are all you know indian respondents so let's take a look at what kind of salaries

246
00:24:17,760 --> 00:24:24,880
gaggle users indian gaggle users are earning in india so let's take a look at q10 which is the

247
00:24:24,880 --> 00:24:30,400
question corresponding to salary what is your current yearly compensation in approximate u.s

248
00:24:30,400 --> 00:24:34,080
dollars so let's take a look at what this data looks like

249
00:24:39,440 --> 00:24:43,040
and basically you see that it gives us a lot of values it's basically

250
00:24:43,840 --> 00:24:49,200
not single values but ranges of values the first respondent is saying that their yearly

251
00:24:49,200 --> 00:24:54,720
compensation in approximate u.s dollars is five thousand dollars to something like seven thousand

252
00:24:54,720 --> 00:25:00,240
four ninety nine dollars the fourth respondent is saying that their approximate compensation in

253
00:25:00,240 --> 00:25:06,960
american dollars is from four to five thousand dollars and so on we also have some nn values

254
00:25:06,960 --> 00:25:12,560
nn here means not a number which means that that was a null value this particular respondent

255
00:25:12,560 --> 00:25:18,560
respondent number 10 chose not to answer this question perhaps so that is why their response

256
00:25:18,560 --> 00:25:24,240
for this field is not number it's basically not applicable it's a null value now one way to simply

257
00:25:24,240 --> 00:25:30,240
calculate how many null values are present in a row is by using this function called is any

258
00:25:30,240 --> 00:25:36,720
basically select the column that you want and apply the is any function when we run that we see

259
00:25:36,720 --> 00:25:45,440
that a bunch of things are true so this is true because the 10th respondent response compensation

260
00:25:45,440 --> 00:25:50,320
is not a number the 14th ones respond response is also not a number and so on so you see that

261
00:25:50,400 --> 00:25:54,720
there are a lot of values here which are true which means that a lot of indians

262
00:25:54,720 --> 00:26:01,920
didn't exactly reveal the approximate salary that they earn in u.s dollars so first what we need to

263
00:26:01,920 --> 00:26:07,040
do is if we have to do an exploration on salaries we have to drop these rows we have to make sure

264
00:26:07,040 --> 00:26:14,000
that we only take those people whose yearly compensation at least in the broad range is

265
00:26:14,000 --> 00:26:18,800
known we want to drop everything that has a nand under this particular column now to see

266
00:26:18,800 --> 00:26:24,720
how many people have these null values we can just add a dot sum in front of the easing so

267
00:26:24,720 --> 00:26:32,080
essentially we take the is any which is going to return this true and false false values for each

268
00:26:32,080 --> 00:26:37,520
row and when we sum them up we are basically going to get the number of people who haven't

269
00:26:37,520 --> 00:26:44,880
responded to this question so let's run that and we say that you know overall number of people who

270
00:26:44,880 --> 00:26:52,480
did not respond to this question is 2252 and the total length of the indian subset of the data is

271
00:26:52,480 --> 00:27:00,000
4729 so you know a little less than half the people have basically not answered this question

272
00:27:00,000 --> 00:27:05,920
and what we need to do is drop all the rows which have na in this particular column na and

273
00:27:05,920 --> 00:27:11,120
this particular column and for that we can use a function called drop any which means drop not a

274
00:27:11,120 --> 00:27:17,120
number of values and we say that only use the quit q 10 column as a subset in the sense that

275
00:27:17,120 --> 00:27:24,160
we are saying that from the india data frame drop all those rows for which q 10 is a null and we

276
00:27:24,160 --> 00:27:29,120
want to do this again in place as earlier we don't want to create a new data frame we want to modify

277
00:27:29,120 --> 00:27:35,280
the existing data frame in place and then when we take the head again of the india data set this is

278
00:27:35,280 --> 00:27:42,240
what we get so you see that now we are not seeing any null values but at the same time we see that

279
00:27:43,040 --> 00:27:48,960
some rows have a dollar sign in front of them you see that this particular row doesn't have a dollar

280
00:27:48,960 --> 00:27:53,760
this particular row doesn't have a dollar but this one does so one of the things that we need to do

281
00:27:53,760 --> 00:27:59,680
in order to perform any sort of mathematical analysis is get rid of that dollar symbol and

282
00:27:59,680 --> 00:28:05,760
see if we can only focus on the numbers itself so we have seen lambda functions before basically

283
00:28:05,760 --> 00:28:10,000
it's just a one line function which takes an input and performs some operation on that input

284
00:28:10,000 --> 00:28:16,480
so basically we are creating a lambda function called remove dollar which takes an input x and

285
00:28:16,480 --> 00:28:21,920
in that x it's going to replace the dollar with an empty string it's basically just going to

286
00:28:21,920 --> 00:28:28,480
remove that dollar then on this subset the india subset we take the q 10 column which corresponds

287
00:28:28,480 --> 00:28:33,920
to compensation and we are going to apply the remove dollar function what this will do is that

288
00:28:33,920 --> 00:28:44,560
for every row this row this row and all the rows all the you know 4729 minus 2252 rows that we've

289
00:28:44,560 --> 00:28:49,920
already dropped on that many number of rows it's going to apply the remove dollar function so when

290
00:28:49,920 --> 00:28:55,120
we do that and run this again we see that this time we get something which doesn't have a dollar

291
00:28:55,120 --> 00:29:01,120
so we see that the 22nd respondent had dollar in front of their output here the dollar is gone

292
00:29:01,120 --> 00:29:06,880
right now another thing we need to do further step in the pre-processing is to remove the commas

293
00:29:06,880 --> 00:29:12,560
because that comma is going to prevent us from interpreting these figures as numbers so the

294
00:29:12,560 --> 00:29:17,360
moment you put in a comma python will not be able to convert that into a number and therefore it

295
00:29:17,360 --> 00:29:22,080
will still treat the whole thing as a string instead of as a number but in order to do statistics

296
00:29:22,080 --> 00:29:26,480
we need numbers so that's why just like we've removed the dollar symbol first now we are going

297
00:29:26,480 --> 00:29:32,000
to remove the comma another way of another alternative to use this particular lambda function

298
00:29:32,000 --> 00:29:39,760
is to use the str dot replace function that is also available on any pandas column so it's simply

299
00:29:39,760 --> 00:29:45,440
going to say that take this particular column and within that wherever you find a comma replace it

300
00:29:45,440 --> 00:29:49,840
with an empty string it's just like the lambda except it's you know one line less than this as

301
00:29:49,840 --> 00:29:54,800
you can see so let's do that and then when we see the head we see that there are no commas

302
00:29:54,800 --> 00:30:00,320
five comma triple zero dash seven comma four double nine has become five thousand hyphen seven four

303
00:30:00,320 --> 00:30:05,200
nine nine so if you were to basically take this value we can interpret that as a number we can

304
00:30:05,200 --> 00:30:10,960
call the int function which we have seen before to convert this into an integer now the next

305
00:30:10,960 --> 00:30:15,920
big problem that we have in cleaning and pre-processing the data is that these are

306
00:30:15,920 --> 00:30:21,040
not single numbers these are ranges of numbers so what we need to do is separate out this column

307
00:30:21,040 --> 00:30:26,400
basically we have a salary column we need a salary minimum which is going to contain the lower bound

308
00:30:26,400 --> 00:30:33,280
five thousand four thousand ten thousand zero hundred thousand and so on as the minimum range

309
00:30:33,280 --> 00:30:38,080
of the values the lower bucket or the lower edge and then we need another column which contains

310
00:30:38,080 --> 00:30:43,120
seven four nine nine four triple nine fourteen triple nine and so on which is the upper edge of

311
00:30:43,120 --> 00:30:48,400
that particular range so what we're going to do is just like we had a str dot replace we also

312
00:30:48,400 --> 00:30:54,160
have an str dot split and here what the split function does is that it takes in any string

313
00:30:54,800 --> 00:31:00,080
and it's going to split every row on that string if that string is found so here the splitter that

314
00:31:00,080 --> 00:31:06,400
we are using is a hyphen so essentially what it's going to do is every time it sees a hyphen it's

315
00:31:06,400 --> 00:31:10,880
going to take everything before it and create a separate element out of it and then it's going

316
00:31:10,880 --> 00:31:16,240
to take everything after it and create a separate element out of that so let's try that so that's how

317
00:31:16,240 --> 00:31:22,480
we see it's basically going to split on whatever splitter we provide and the remaining parts it's

318
00:31:22,480 --> 00:31:27,520
basically going to convert into a list so five thousand hyphen seven four double nine became a

319
00:31:27,520 --> 00:31:31,840
list which contains five thousand as the first element and seven four double nine as the second

320
00:31:31,840 --> 00:31:38,160
element and the same process has been carried over for the entire range so let's try that so

321
00:31:38,640 --> 00:31:44,560
in order to keep this data as a new column in our data frame we are going to create a new column

322
00:31:44,560 --> 00:31:50,240
called salary range and that we are going to simply assign to this particular function we

323
00:31:50,240 --> 00:31:59,680
are going to take a column and call str dot split we are simply going to create a new

324
00:31:59,680 --> 00:32:05,440
column called salary range which takes the q10 column and calls the str dot split with the hyphen

325
00:32:05,520 --> 00:32:12,080
as the delimiter the splitter so let's do that and when we look at the head of this function

326
00:32:12,080 --> 00:32:18,560
this is what we get so now this is present as a new column in the india data frame itself so now

327
00:32:18,560 --> 00:32:23,760
what we need to do is get the min and maximum salaries from this range so for example here

328
00:32:23,760 --> 00:32:29,360
the minimum salary is five thousand the maximum salary is seven four nine now that also we can

329
00:32:29,360 --> 00:32:36,000
be doing with a lambda function essentially remember that this is a list and what we are

330
00:32:36,000 --> 00:32:41,520
going to do is apply a lambda function which takes the zeroth element of the list and then

331
00:32:41,520 --> 00:32:45,680
takes the first element of the list now we know that zeroth element of the list is the minimum

332
00:32:45,680 --> 00:32:52,320
salary so we simply call it s min and the first element of the list is basically s max which is

333
00:32:52,320 --> 00:32:58,640
the upper bound of the salary so let's run that and we see that something has failed it's basically

334
00:32:58,640 --> 00:33:05,600
said that the list index is out of range what has gone wrong here let's take a look one of the

335
00:33:05,600 --> 00:33:12,720
these are you know the reasons why it might have gone wrong first is that we didn't write the code

336
00:33:12,720 --> 00:33:17,200
properly which would have resulted in incorrect syntax but that doesn't seem to be the problem

337
00:33:17,200 --> 00:33:22,240
because it has given me a specific index error if it was a syntax error it would have said that it's

338
00:33:22,240 --> 00:33:26,640
a syntax error there might be something like missing values in the column that is also not

339
00:33:26,640 --> 00:33:33,840
the case because we have already dropped all the missing values when we did the drop in it right

340
00:33:35,680 --> 00:33:39,280
then the third option is that not all rows have two elements in the salary

341
00:33:40,160 --> 00:33:46,240
range column now you see that in order for this lambda to work i need the zeroth element as i as

342
00:33:46,240 --> 00:33:50,800
well as you know i need the first element and if you look at this particular error carefully you

343
00:33:50,800 --> 00:33:55,200
see that it has failed at the second part that's what the arrow is showing so this particular

344
00:33:55,200 --> 00:34:00,000
function ran perfectly fine which means that after the split is done it is able to find the

345
00:34:00,000 --> 00:34:05,280
zeroth element all the time but in some cases it's not able to find the first element the only reason

346
00:34:05,280 --> 00:34:10,240
that can happen is that the string contains only one element some cases it just doesn't contain

347
00:34:10,240 --> 00:34:16,960
anything so let's see what happens to those cases where the string did not contain two elements

348
00:34:16,960 --> 00:34:22,800
well now we know that the salary range column each row is basically a list earlier each row

349
00:34:22,800 --> 00:34:28,160
was a string which we have converted into a list by carrying out the split function so basically

350
00:34:28,160 --> 00:34:33,920
each row is going to be a list now and we know how to calculate the length of a list we can just

351
00:34:33,920 --> 00:34:38,800
do len and the list so what we are going to do is apply the length function to this and we are going

352
00:34:38,800 --> 00:34:45,760
to capture the output in l and when we see the minimum length of the lists that are available

353
00:34:45,760 --> 00:34:49,760
so this has a length of two this has a length of two all of these have a length of two right so we

354
00:34:49,760 --> 00:34:55,280
would expect throughout the data frame for every list within the salary range column to have a

355
00:34:55,280 --> 00:35:01,040
length of two right but that's not what is happening if we look at the minimum of list we see that there

356
00:35:01,040 --> 00:35:06,880
are definitely some lists which have a length of only one now let's see why this happened one of

357
00:35:06,880 --> 00:35:15,360
the reasons why that could happen is like that let's see you see that we have this data set

358
00:35:15,360 --> 00:35:19,680
where the salary range is simply greater than five thousand and the s min is simply greater

359
00:35:19,680 --> 00:35:24,240
than five thousand now you see that these are rows which don't contain a hyphen at all

360
00:35:24,240 --> 00:35:28,560
we were counting on that hyphen to be able to split the minimum salary and the maximum salary

361
00:35:28,560 --> 00:35:33,280
but if there is no hyphen if your salary is actually more than five hundred thousand us

362
00:35:33,280 --> 00:35:38,000
dollars then there is no such thing as a raise there isn't like a minimum hyphen maximum there's

363
00:35:38,000 --> 00:35:44,000
just greater than five thousand right so that is why we did not end up with a minimum value at all

364
00:35:44,000 --> 00:35:50,160
so in order to get the maximum salary we already have the minimum salary which is simply our s min

365
00:35:50,160 --> 00:35:54,480
now in order to get the maximum salary what we have to do is modify our lambda function a little bit

366
00:35:55,520 --> 00:36:00,160
so now this is the function that failed it simply said that get the first value which is

367
00:36:00,160 --> 00:36:04,240
this particular value out of it now of course some of them didn't have the hyphen at all so

368
00:36:04,240 --> 00:36:07,760
you know it might have ended up with a list with a single element element so there was no first

369
00:36:07,760 --> 00:36:12,960
element there was only the zeroth element so what we are going to do is modify the lambda function

370
00:36:12,960 --> 00:36:18,960
such that we say that if the length of x is two then give us x1 otherwise give us the zeroth value

371
00:36:18,960 --> 00:36:25,040
so let's do that and then we have a s min x max this time it ran properly by the way these things

372
00:36:25,040 --> 00:36:29,280
that you keep seeing these are just warnings they are not errors and that's okay it's okay to ignore

373
00:36:29,280 --> 00:36:36,720
them for now and then when we look at s min these are now the individual objects that we see and

374
00:36:36,800 --> 00:36:44,640
these are pure numbers they are still objects but they are at least able to be converted into

375
00:36:44,640 --> 00:36:50,480
numbers we can just do an as type int as we did earlier and then convert them into numbers

376
00:36:50,480 --> 00:36:57,520
whereas you see in the max column again we'll still have this greater than five thousand

377
00:36:57,520 --> 00:37:02,720
side so a greater than you know five hundred thousand now what we need to do is get rid of

378
00:37:02,720 --> 00:37:06,960
that greater than five thousand and cap it there now if it's greater than five hundred thousand we

379
00:37:06,960 --> 00:37:11,840
don't know how how large it can go so let's just use whatever we have let's use the upper bound as

380
00:37:11,840 --> 00:37:18,000
the maximum salary itself and what we're going to do then is simply replace again like earlier we

381
00:37:18,000 --> 00:37:23,200
replace the dollar or the comma we replace the greater than sign with an empty string and so

382
00:37:23,200 --> 00:37:27,200
there it is now we have an s min let's take a look at

383
00:37:34,480 --> 00:37:41,680
salary range the salary min and the salary max columns and there we are so we have the

384
00:37:41,680 --> 00:37:46,560
salary range which we computed by splitting five thousand becomes the minimum salary

385
00:37:46,560 --> 00:37:51,360
and seven four nine nine becomes the maximum salary let's also take a look at the

386
00:37:51,360 --> 00:37:58,240
bottom few rows just like head shows the top five rows tail shows the bottom five rows so there it is

387
00:37:58,240 --> 00:38:03,760
you see zero is the minimum salary 9.99 is the first salary so minimum maximum becomes a little

388
00:38:03,760 --> 00:38:09,840
bit like that now let's see if we can convert the s min column here into integers for that we can

389
00:38:09,840 --> 00:38:15,360
use the as type int and as earlier we can simply use the hist function to check the histogram so

390
00:38:15,360 --> 00:38:22,320
now we have managed to get a numerical column out of a bunch of categorical columns so let's run that

391
00:38:23,280 --> 00:38:29,600
and this is what we see we see that most people have a salary of less than what is it

392
00:38:33,360 --> 00:38:38,880
50 000 us dollars so basically that makes sense and very few people have salaries that are more

393
00:38:38,880 --> 00:38:46,240
than 50 000 us dollars some of them have all the way up to 500 000 so there are definitely the

394
00:38:46,240 --> 00:38:50,800
disparity you see that there is a lot of room at the there is a lot of crowd at the bottom a lot

395
00:38:50,800 --> 00:38:57,840
of people are earning less than 50 000 per year well by indian standards that may be a good amount

396
00:38:57,840 --> 00:39:03,760
of money but remember that there are still a bunch of people far ahead of that particular curve now

397
00:39:03,760 --> 00:39:10,160
in order to see this a little more you know in detail we simply increase the number of bins this

398
00:39:10,160 --> 00:39:17,200
is 10 by default and we are just going to increase it by the bins to 50 and we are going to limit our

399
00:39:17,200 --> 00:39:24,240
x-axis to 0 and 100 000 so basically our x-axis goes all the way from 0 to 500 000 we just want

400
00:39:24,240 --> 00:39:30,080
to since most of our data is concentrated in this particular bin we are setting the x limit so this

401
00:39:30,080 --> 00:39:36,480
is what the set xlim function does it basically sets the x limit to between 0 and 100 000 and

402
00:39:36,480 --> 00:39:42,480
then we are also changing the number of bins so there we are so this is basically how it looks like

403
00:39:42,480 --> 00:39:46,880
most of the data as we saw is concentrated between these two bins but if we magnify that

404
00:39:46,880 --> 00:39:51,440
particular limit and increase the number of bins this is what we see again so most people again

405
00:39:51,520 --> 00:39:56,320
like roughly 1400 people are earning less than or equal to

406
00:39:58,480 --> 00:40:04,880
10 000 per annum and fewer and fewer people keep on getting higher salaries and that

407
00:40:04,880 --> 00:40:09,360
makes sense this is also called a Pareto distribution or a power law distribution

408
00:40:09,360 --> 00:40:14,880
where most people will be earning very will be earning lower salaries and as the salaries

409
00:40:14,880 --> 00:40:20,720
goes on increasing more and more people start you know fewer and fewer people rather start

410
00:40:20,720 --> 00:40:25,840
earning those higher amounts of salary right so then how do we examine some other non-numerical

411
00:40:25,840 --> 00:40:32,240
columns remember that this is still univariate analysis let's take a look at one other non-numerical

412
00:40:32,240 --> 00:40:38,960
column or a categorical column q5 which is select the title which is most similar to your current

413
00:40:38,960 --> 00:40:44,560
role this is basically your designation or your job choice or your job title and let's take a look

414
00:40:44,560 --> 00:40:49,440
so we have things like software engineer other business analysts data scientists software

415
00:40:49,440 --> 00:40:55,200
engineer and so on if you look at the value counts we see that there are these many unique

416
00:40:56,160 --> 00:41:02,720
designations within the indian data set most frequently the ones that are occurred are

417
00:41:02,720 --> 00:41:08,240
software engineer and data scientists of course this being data from kaggle which is a platform

418
00:41:08,240 --> 00:41:14,080
primarily for data scientists it's not surprising that a lot of people show up here what is

419
00:41:14,160 --> 00:41:20,240
surprising though is there are 600 more than 600 data scientists there are more than 600 software

420
00:41:20,240 --> 00:41:26,320
engineers but there are only 300 data analysts so what this means is that perhaps data analyst is

421
00:41:26,320 --> 00:41:34,240
not a very popular job title amongst people other is just sort of a catch-all term for you know any

422
00:41:34,240 --> 00:41:38,560
other job title that you may have which doesn't fall into any of the other categories there are

423
00:41:38,560 --> 00:41:45,200
very few database engineers and administrators that is well that's not surprising now if i

424
00:41:45,200 --> 00:41:50,320
wanted to see the proportion of all of these one of the quick ways i could do that is you've seen

425
00:41:50,320 --> 00:41:55,680
that we have a plot function and we used it to plot a bar chart and a pie chart before so we're

426
00:41:55,680 --> 00:42:01,600
just going to plot the bar chart again on this particular value counts output so there it is

427
00:42:01,600 --> 00:42:07,280
basically it tells me that there are about 600 odd data scientists and data engineers but half of

428
00:42:07,280 --> 00:42:13,440
them in fact it's really weird that the top two most frequently occurring

429
00:42:13,440 --> 00:42:17,920
designations are data scientists and software engineers and the second designation and the

430
00:42:17,920 --> 00:42:24,080
third most frequent designation that occurs is almost half of the first two which is weird

431
00:42:24,080 --> 00:42:30,000
usually you'll see some spot of a smooth curve over here but this is interesting that's kind of

432
00:42:30,000 --> 00:42:35,680
an anomaly this means that you know kaggle in india is more or less completely dominated by

433
00:42:35,680 --> 00:42:40,240
data scientists and software engineers now you see this isn't very readable so you can also plot

434
00:42:40,240 --> 00:42:45,920
a bar edge which is a horizontal bar chart that creates something like this so this is slightly

435
00:42:45,920 --> 00:42:50,400
more readable because you can read the titles and you know the length of the bars anyway indicate

436
00:42:50,400 --> 00:42:56,560
the proportion in which they are present so these are two independent kinds of univariate analysis

437
00:42:56,560 --> 00:43:02,320
we have done we took a numerical column well we manufactured first a numerical column we saw its

438
00:43:02,320 --> 00:43:07,920
histogram we saw what the behavior of salaries is and then we simply did counting on a categorical

439
00:43:07,920 --> 00:43:13,600
column and saw the proportion of them but here's an example of a bivariate analysis which i will

440
00:43:13,600 --> 00:43:20,160
start as an exercise which job title has the highest average salary so think about this how

441
00:43:20,160 --> 00:43:24,080
are we going to do this basically we want to say that get all the data scientists compute their

442
00:43:24,080 --> 00:43:29,440
salaries get the average get all the software engineers compute the average salary and report it

443
00:43:29,440 --> 00:43:33,680
and that's what we want to do for every job title and then we want to figure out

444
00:43:33,680 --> 00:43:38,400
which job title has the highest average salary now of course what do we mean by salary we in

445
00:43:38,400 --> 00:43:44,480
fact have two columns s we add s min and x s max so which of them do we consider as the salary

446
00:43:44,480 --> 00:43:50,000
well let's take s min because that way we are you know preparing an analysis for the worst

447
00:43:50,000 --> 00:43:55,760
possible scenario that is if we find a reasonable output even by considering s min then we know that

448
00:43:55,840 --> 00:44:02,800
even if you are at the lower edge of whatever you could be earning it's still a good outlook

449
00:44:02,800 --> 00:44:09,520
so let's simply take s min as the salary s max is a little optimistic let's go with you know

450
00:44:10,160 --> 00:44:15,920
the very conservative or the worst case possible approach over here so let's say that s min is the

451
00:44:15,920 --> 00:44:20,640
actual salary that you're earning and then we want to figure out given that s min is the salary

452
00:44:20,640 --> 00:44:23,600
which particular job title has the most

453
00:44:26,000 --> 00:44:29,920
has the highest average salary and now what do we mean by average again then

454
00:44:29,920 --> 00:44:35,280
is that to reiterate we pick a particular title let's say data scientist find all the data

455
00:44:35,280 --> 00:44:43,440
scientists in that particular data frame calculate the average of all their salaries which is s min

456
00:44:43,440 --> 00:44:49,520
and then find out which of these particular roles has the highest average so think about this try

457
00:44:49,520 --> 00:44:54,080
to break it down into smaller steps and then we'll see about how to solve this feel free to pause

458
00:44:54,080 --> 00:45:01,440
the video look around and come back when you have a solution well let's get started and see how we

459
00:45:01,440 --> 00:45:06,880
can basically break this down well one of the first thing we need to do is capture in some variable

460
00:45:06,880 --> 00:45:12,080
all of the different job titles that we already have we can do that straight away by looking at

461
00:45:12,080 --> 00:45:17,600
this variable so there it is these are all the different titles we have and these are their

462
00:45:17,600 --> 00:45:24,720
accounts so let's just call it this job let's call this particular variable job title counts

463
00:45:25,360 --> 00:45:33,200
equal to the value counts of the q5 column so then job title counts this is what it looks like

464
00:45:33,200 --> 00:45:39,600
then what i need to do is filter the original data set by each of these values take the s min

465
00:45:39,600 --> 00:45:44,000
value out of them sum them up and basically divide it by this number because that's the

466
00:45:44,000 --> 00:45:49,200
number of times it appears so that would then give me the average so what i can do is i want

467
00:45:49,200 --> 00:45:55,200
to iterate over this value now one of the things about iterating over a particular series is that

468
00:45:55,200 --> 00:45:59,360
remember that it basically behaves like a dictionary so it would be something like

469
00:45:59,360 --> 00:46:07,200
for job title which is the key and then count which is the value in job title counts

470
00:46:07,200 --> 00:46:20,240
dot vector items and i can just do print job title and count just to ensure that my iteration

471
00:46:20,240 --> 00:46:24,720
is happening properly and it seems like it is i have the job title and i have the count against

472
00:46:24,720 --> 00:46:30,560
it so my job title and count variables are in the iterative loop are working correctly as expected

473
00:46:30,640 --> 00:46:38,320
so what i can do is create a small df which is something like i take xdf and then i take the

474
00:46:38,320 --> 00:46:45,600
q5 column such that it matches this particular job title so basically i have the small df which is

475
00:46:45,600 --> 00:46:49,280
a subset of the oops sorry this should not be x this should be india

476
00:46:52,000 --> 00:46:57,840
so out of the india slice of the data set we want to take the fifth column and filter it with the

477
00:46:57,840 --> 00:47:03,200
particular job title and we are of course iterating on all of them so that small df and

478
00:47:03,200 --> 00:47:09,120
let's just break this loop over here to find out if you know we are actually doing this iteration

479
00:47:09,120 --> 00:47:14,160
correctly so what break does is that after just one step of iteration it's just going to pick

480
00:47:14,160 --> 00:47:22,000
okay so let's do that and let's take a look at small df dot head nothing special but you see that

481
00:47:22,000 --> 00:47:26,640
q5 is basically data scientist and that makes sense the first item was data scientist it just

482
00:47:26,640 --> 00:47:31,280
took you know the data scientist in the first level of iteration and broke the data set so this

483
00:47:31,280 --> 00:47:37,520
is my data frame for all the people who have the designation of a data scientist now what i'm going

484
00:47:37,520 --> 00:47:46,640
to do is simply pick the salary column which is small df and then i can do just smin because

485
00:47:46,640 --> 00:47:52,080
earlier as we decided we are going to use smin as the salary and then i'm just going to do a

486
00:47:52,080 --> 00:47:57,040
salary dot sum you remember that's how you do a salary but let's break it anyway and see if we

487
00:47:57,040 --> 00:48:03,360
can get the salary column properly so if you look at salary here's what we get so basically the

488
00:48:03,360 --> 00:48:10,080
sixth person in the data set has a salary of 10,000 the 76 percent has a salary of 20,000

489
00:48:10,080 --> 00:48:15,440
and so on some people have 5,000 some people have 4,000 and so on i can just do a salary dot sum to

490
00:48:15,440 --> 00:48:23,200
get the totals oh that didn't work you see i wanted to basically do some of the salary but it all

491
00:48:23,200 --> 00:48:29,920
you know got converted into a big string which means that the salaries that we're looking at

492
00:48:29,920 --> 00:48:35,680
are not basically integers when i do a salary dot head it basically says that the data type is object

493
00:48:35,680 --> 00:48:42,000
well in that case let's just do a salary dot as type integer which is fine and then when i do a

494
00:48:42,720 --> 00:48:49,040
sum gives me something which is a number which is not that crazy string that it gave me earlier

495
00:48:49,040 --> 00:48:53,200
what it did earlier was that it simply concatenated all the strings which is why i was getting a big

496
00:48:53,200 --> 00:48:57,680
string so when you do a sum on a column that contains string it's going to concatenate the

497
00:48:57,680 --> 00:49:02,160
strings it's not going to produce the numerical result even if the string actually represents

498
00:49:02,160 --> 00:49:08,000
a number that is simply because we haven't done this as type integer step so there it is i can

499
00:49:08,080 --> 00:49:15,680
simply add this over here i can do something like as type integer and then i can say

500
00:49:18,160 --> 00:49:26,880
total salary for title is salary dot sum and then i can say average

501
00:49:29,520 --> 00:49:31,520
salary for title

502
00:49:31,520 --> 00:49:43,120
is equal to well total salary for title divided by the count because counts i am getting from here

503
00:49:43,120 --> 00:49:51,200
anyway right that becomes my average salary now in order to contain all this data i'll create

504
00:49:51,200 --> 00:49:57,840
a dictionary let's just call it salaries which is an empty dictionary what i'm going to do is that

505
00:49:57,840 --> 00:50:04,080
i'm going to put the designation inside the dictionary which is job title and the value

506
00:50:04,080 --> 00:50:09,440
is going to be the average salary for the title so let's run this and print what we get in

507
00:50:10,560 --> 00:50:18,080
salads there is a name error it says that salary is not defined well it's not salary it's salaries

508
00:50:19,360 --> 00:50:26,240
so there it is we have corrected the name and we see that the average salary of a data scientist

509
00:50:26,240 --> 00:50:31,200
in india is twenty three thousand dollars that of a software engineer is much lower which is

510
00:50:31,200 --> 00:50:36,800
thirteen thousand dollars that of a data analyst is well surprisingly less than that of a software

511
00:50:36,800 --> 00:50:41,680
engineer which is thirteen thousand three seventy five it's a difference of five hundred dollars

512
00:50:41,680 --> 00:50:47,920
per year which is still a sizable amount and there is the other field which has a high salary

513
00:50:47,920 --> 00:50:51,920
but then you know the other you know it's just a collection of whatever fields didn't already

514
00:50:52,800 --> 00:50:58,400
now you see that this is basically a dictionary one of the things i could simply do is put that

515
00:50:58,400 --> 00:51:08,480
dictionary back into a series and plot it as a bar chart again

516
00:51:11,760 --> 00:51:16,960
so there it is so we see that product managers and project managers are the most followed by data

517
00:51:16,960 --> 00:51:21,520
scientists followed by research scientists followed by business analysts followed by

518
00:51:21,520 --> 00:51:28,000
statisticians and so on so that's what our data looks like now again in the spirit of keeping

519
00:51:28,000 --> 00:51:33,680
things modular and fast all of this jugglery can be avoided so let's see how we can basically change

520
00:51:33,680 --> 00:51:40,320
our code you see that here we calculated the sum then divided it by count this was not really

521
00:51:41,200 --> 00:51:45,120
instead of sum i can just call it mean and then this becomes my

522
00:51:48,240 --> 00:51:53,120
average salary for title of course that as type int i've already done so this should work

523
00:51:53,120 --> 00:51:59,200
and then i can completely get rid of this time so i have that and then again i get the same plot

524
00:51:59,200 --> 00:52:04,640
so this works well one of the other things we could straight away do is use the group by

525
00:52:04,640 --> 00:52:09,200
statement what we are doing is we have covered by the way group by an aggregation in one of the

526
00:52:09,200 --> 00:52:14,880
earlier videos what we are going to do is we are going to take india and we are going to group it

527
00:52:20,400 --> 00:52:25,200
by the q5 column what this means is that it's going to group the entire data set by each unique

528
00:52:25,200 --> 00:52:32,880
value present in the q5 column out of that those groups we are going to select the salary min

529
00:52:32,880 --> 00:52:38,880
column we are going to convert it into an integer and then we are going to simply take the mean of

530
00:52:38,880 --> 00:52:46,080
it so that's how group by an aggregation works and there is a problem okay so apparently you

531
00:52:46,080 --> 00:52:54,720
cannot access callable attribute as type so let's do something else we'll just convert smin into an

532
00:52:54,720 --> 00:53:06,160
integer beforehand so it's basically not letting me convert integer types as part of the group by

533
00:53:06,160 --> 00:53:12,880
operation so i'm just going to move the as type and outside and i'm going to then calculate the

534
00:53:12,880 --> 00:53:19,040
grouped means of the salary column separately so there we are and we see we have more or less the

535
00:53:19,040 --> 00:53:28,560
same data however we can also plot this as a bar chart and here we have so there it is of course

536
00:53:28,560 --> 00:53:35,120
it's occurring a little weirdly if i wanted to basically make this entire chart a little

537
00:53:35,120 --> 00:53:40,720
more readable i would basically put this data in a variable called salaries and then what i would do

538
00:53:40,720 --> 00:53:49,120
is i would do sort values on salaries so that i get a nice readable chart like this where i

539
00:53:49,120 --> 00:53:58,880
go from low to high and then i would basically do a plot and there we are so yeah this is a lot

540
00:53:58,880 --> 00:54:04,720
more readable we see that product and project managers on the most data scientists on

541
00:54:06,480 --> 00:54:10,080
the second most but you see that there is a big drop so you see managers are

542
00:54:10,960 --> 00:54:16,720
a lot more than most other people you have research scientists business analysts

543
00:54:18,000 --> 00:54:22,240
who earn a fair amount of money right this is basically the order in which they go

544
00:54:23,280 --> 00:54:27,440
now you see that the people who earn the least amount of money in this whole thing are data

545
00:54:27,440 --> 00:54:34,080
analysts but if you think about it really carefully a business analyst and a data analyst

546
00:54:34,080 --> 00:54:39,280
are more or less the same thing now this is a you know a hack that you might want to learn from

547
00:54:40,960 --> 00:54:46,240
your job title really matters on your CV so basically if you are a data analyst try to

548
00:54:46,240 --> 00:54:52,400
call yourself a business analyst wherever possible because that alone seems to indicate a much higher

549
00:54:52,400 --> 00:54:55,040
minimum salary remember that we are still talking about s min

550
00:54:56,320 --> 00:55:04,240
so these are the lower worst case scenarios so to be very frank business analysts and data analysts

551
00:55:04,240 --> 00:55:09,360
they don't do things that are very widely different they do things that are fairly

552
00:55:09,360 --> 00:55:17,040
similar to each other so that is why if i mean the point is again that job titles matter and

553
00:55:17,040 --> 00:55:21,840
precisely how they matter well this is how they matter this is basically the average

554
00:55:21,840 --> 00:55:26,960
salary of each job title that you have now there are a lot of other columns present in the data

555
00:55:26,960 --> 00:55:32,240
set you can look at them there are columns that talk about education so you can also repeat the

556
00:55:32,240 --> 00:55:37,120
same exercise and say what sort of education qualification earns the most salaries what sort

557
00:55:37,120 --> 00:55:42,480
of software tools or expertise with certain software tools is likely to get you more salaries

558
00:55:42,480 --> 00:55:48,400
whatever does education correlate with titles so how many data scientists have a PhD for example

559
00:55:48,400 --> 00:55:54,160
so you can do all sorts of these bivariate analysis combinations and i encourage them

560
00:55:54,160 --> 00:55:59,760
i encourage you to try them out so that is probably how you do univariate and bivariate

561
00:55:59,760 --> 00:56:05,680
analysis to tell stories with data essentially here we have a clear-cut understanding of how

562
00:56:05,680 --> 00:56:11,440
salaries are reflected across multiple job titles so do take a look on your own

563
00:56:12,400 --> 00:56:16,000
with more and more columns in the data set and try to see what you come up with

564
00:56:18,400 --> 00:56:18,900
you

